---
{"dateCreated":"2023-01-26 14:42","tags":["probability","computer_science"],"pageDirection":"rtl","dg-publish":true,"permalink":"/computer-science/probability/further-topics-on-random-variables/","dgPassFrontmatter":true}
---



# קונבולוצייה, שונות משותפת , פונקציות על מ״מ ועוד


נרצה לגעת בסוגיות נוספות שעולות כשמדברים על משתנים מקריים בין היתר 

א) חישוב פונקציות הצפיפות של פונקצייה על מ״מ או כמה.
ב) חישוב סכום של משתנים מקריים בלתי תלויים
ג) כימות מידת התלות בין שני משתנים אקראיים

כשיש לנו את המטרות האלה בראש, נכיר מספר כלים שיעזרו לנו כמו טרנפוסמצייה וקונבולוצייה, כמו כן נבין יותר לעומק את המשמעות של תוחלת מותנת.


## פונקציות על מ"מ
נניח ש $Y=g(X)$ כאשר $X$ הוא [[Computer Science/Probability/Continuous Random Variables\|משתנה מקרי רציף]] נרצה לחשב את ה $PDF$ של $Y$ בהינתן זה של $X$ . שתי השלבים  הדרושים כדי לבצע את התהליך הזה:

א) חישוב ה $CDF$ של $Y$ :

$$F_{Y}(y)= P(g(x)\leq y)= \int_{\{x \  | \ g(x) \leq y\}}f_{X}(x)dx$$
ב) חשב לפי חוק הגזירה

$$f_{Y}(y)= \frac{dF_{Y}}{dy}(y)$$


__דוגמה 1:__ 
יהי $X$ משתנה אחיד ב $[0,1]$ ו $Y=\sqrt{X}$ .  יתקיים בקטע הנתון:

$$F_{Y}(y)= P(Y\leq y)= P(\sqrt{X}\leq y)= P(X\leq y^{2})=y^{2}$$
אין כלל צורך לחשב את האינטגרל כיוון שאנחנו יודעים ש בכל הקטע $[0,1]$ יקיים את הנ״ל ולכן אנחנו פשוט מבצעים אינטגרצייה על $f_{X}=1$ בקטע הזה ונקבל כמובן $x=y^{2}$ .

כעת לאחר גזירה נקבל 

$$f_{Y}(y)= F_{Y}^{\prime}(y)= 2y \ \ \ \ \ 0\leq y\leq 1$$
מחוץ לקטע הזה מתקיים ש $F_{Y}(y)=0$ עבור $y\leq  0$ ועבור $y\geq 1$  מתקיים :$F_{Y}(y)=1$ מכאן ש $f_{Y}(y)=0$  מחוץ לקטע.

__דוגמה 2:__
ג׳ון האיטי נוסע מבוסטון לניו יורק , מרחק של 180 קילומטר במהירות קבועה שהערך שלה מתפלג באופן אחיד בין 30 ל 60 קילומטר לשעה (כלומר יש התפלגות אחידה בין המהירות שיכולות להיות לו בטווח הזה). נרצה לחדשה את ה PDF של זמן הנסיעה. 
נסמן את $X$ כמהירות ו $Y=g(X)$ ונסמן את $Y=g(X)$ כמשך הזמן של הטיול כלומר 

$$g(X)= \frac{180}{X}$$
כדי לחשב את ה $CDF$ של $Y$ עלינו לחשב 

$$P(Y\leq y)= P\left( \frac{180}{X}\leq y\right)= P\left( \frac{180}{y}\leq X\right)$$

נשים לב שאנחנו יודעים לחשב את פונקציית צפיפות ההסתברות של $X$ בקלות בגלל שהיא מתפלגת בצורה אחידה

$$f_{X}(x)= \begin{cases} \frac{1}{30}& x\in[30,60] \\ 0 & else\end{cases}$$
וה $CDF$ של $X$ יהיה

$$F_{X}(x)= \begin{cases} 0 & x\leq 30 \\ \frac{x-30}{30} & x\in [30,60] \\ 1& x\geq 60\end{cases}$$
אם כן :

$$P\left( \frac{180}{y}\leq X\right)= 1- P\left(X < \frac{180}{y}\right)= 1- F_{X}\left( \frac{180}{y}\right)$$
נוכל להתאים את הערך $\frac{180}{y}$ לטווחים הרצויים ונקבל 

$$\begin{cases} 0 & y\leq \frac{180}{60}= 3\\ 1- \frac{\frac{180}{y}-30}{30}& y\in [\frac{180}{60}, \frac{180}{30}]= [3,6] \\ 1 & else\end{cases}$$
האמצעי זה שקול ל $1- \frac{6}{y}$ ולכן אם נבצע גזירה נקבל 

$$f_{Y}(y)= \begin{cases} 0 & y\leq 3 \\ \frac{6}{y^{2}}& y\in (3,6) \\ 0 & y\geq 6\end{cases}$$


![Pasted image 20230126193716.png|350](/img/user/Assets/Pasted%20image%2020230126193716.png)

__דוגמה 3:__ 
נגדיר $Y=g(X)=X^{2}$ כאשר $X$ הוא עם PDF ידוע. יתקיים שלכל $y\geq 0$ :
$$F_{Y}(y)= P(Y\leq y) = P(X^{2}\leq y)= P(-\sqrt{y}\leq X\leq \sqrt{y})= F_{X}(\sqrt{y})- F_{X}(-\sqrt{y})$$

אם נבצע גזירה נקבל

$$f_{Y}(y)= \frac{1}{2\sqrt{y}} f_{X}(\sqrt{y}) + \frac{1}{2\sqrt{y}}f_{X}(-\sqrt{y})$$


### פונקציות ליניאריות 
נתמקד במקרה המיוחד שבו $Y$ היא פונקצייה ליניארית של $X$ 

![Pasted image 20230126194008.png|450](/img/user/Assets/Pasted%20image%2020230126194008.png)

ה $PDF$ של $ax+b$ במונחים של ה $PDF$ של $X$ . בתמונה למעלה $a=2,b=5$ . בשלב הראשון מוצאים את $PDF$ של $Y=aX$ . יתקיים כתוצאה מכפל בסקלר ש $Y$ מקבל טווח ערכים רחב יותר מהטווח של $X$ בפקטור $a$ . כלומר ישנה ״מתיחה״ לאורך ציר ה $x$ אבל בגלל שצריך לשמור על תכונת הנורמליזצייה צריך לנרמל את הערכים החוזרים מהפונקצייה ולכן ה״גובה״ שלה קטן. הוספת הערף $b$ לא תשפיע על הטווח של הפונקצייה אלא רק תזיז את ערכיו  לאורך ציר ה $x$ ב $b$ . בסופו של דבר יתקבל הביטוי המתמטי 

$$f_{Y}(y)= \frac{1}{|a|}f_{X}\left( \frac{y-b}{a}\right)$$
_בהינתן ש a שונה מ0 כמובן_

__הוכחה__:
בלי הגבלת הכלליות נניח $a>0$ כאשר המקרה על $a<0$ דומה 

$$F_{Y}(y)= P(Y\leq y)= P(aX+b\leq y)= P\left(X\leq \frac{y-b}{a}\right)= F_{X}\left(\frac{y-b}{a}\right)$$
על ידי גזירה נקבל 

$$f_{Y}(y)= \frac{dF_{Y}}{dy}(y)= \frac{1}{a}f_{X}\left( \frac{y-b}{a}\right)$$
המקרה ש $a<0$  פשוט יוביל לכך שהסימן למעלה יתהפך ונגזיר את $1-F_{X}$ אבל החיסור לא ישפיע על הנגזרת (אומנם נקבל מינוס לפני התוצאה אבל בגלל ש $a$ שלילי זה יקזז את זה וכאן נכנס הערך המוחלט מלמעלה).

#### פונקצייה על ליניארית על משתנה מקרי מעריכי
נניח ש $X$ הוא משתנה [[Computer Science/Probability/Continuous Random Variables#משתנה מקרי מעריכי\|מתפלג מעריכית]]  עם $\lambda>0$ שמקיים 
 
$$f_{X}(x)= \begin{cases} \lambda e^{-\lambda x} & x\geq 0 \\ 0& else\end{cases}$$
נניח ש $Y=aX+b$ אזי לפי מה שהראנו למעלה 

$$f_{Y}(y)= \begin{cases} \frac{\lambda}{|a|} e^{-\lambda(\frac{y-b}{a})}& \frac{y-b}{a}\geq 0 \\ 0& else\end{cases}$$

נשים לב שאם $b=0$  ו $a$ חיובי אזי $Y$ הוא גם כן משתנה מתפלג מעריכית כאשר $\lambda_{y}=\lambda\cdot a$ . באופן כללי אבל, $Y$ אינו מתפלג מעריכית למשל אם $a<0$ ו $b=0$ אז טווח הערכים של $Y$ הוא ציר ה $x$  השלילי .

#### פונקצייה ליניארית על משתנה מקרי נורמלי
נניח ש $X$ הוא [[Computer Science/Probability/Normal Random Variable\|משתנה מקרי נורמלי]] עם תוחלת $\mu$ ושונות $\sigma^{2}$ ויהי $Y=aX+b$ עם סקלרים $a\neq0,b$ אזי 
$$f_{X}(x)= \frac{1}{\sqrt{2\pi}\sigma} e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}$$
סך הכל נקבל 

$$\displaylines{
f_{Y}(y)= \frac{1}{|a|}f_{X}\left( \frac{y-b}{a}\right) \\ 
= \dots \\ = \frac{1}{\sqrt{2\pi }|a|\sigma} e^{- \frac{(y-b-a\mu)^{2}}{2a^{2}\sigma^{2}}}
}$$

שזה גם כן משתנה מתפלג נורמלי עם תוחלת $a\mu +b$ ושונות $a^{2}\sigma^{2}$ .

### פונקציות מונוטוניות
נוכל להכליל את המקרה הליניארי עבור המקרה ש $g$ היא מונוטונית. יהי $X$ משתנה מקרי רציף שתחום הערכים שלו מוכל ב $I$ אינטרוול כלשהו, כלומר $x\notin I\rightarrow f_{X}(x)=0$ .
כעת נגדיר $Y=g(X)$ כך ש $g$ __מונוטונית ממש__ (עולה או יורדת) ונגדיר אותה גם כגזירה (נשים לב שהנגזרת בהכרח תהיה אי שלילית במקרה העולה ו אי חיובית במקרה היורד).

נוכל להוכיח באמצעות [[Computer Science/Discrete Math/Cardinal number\|עוצמות]] שפונקציות אלה הן הפיכות תמיד כלומר קיימת פונקצייה הופכית ל $g$ כך 
$$y=g(x)\leftrightarrow x= h(y)$$
ו $h$ היא ההופכית. 

אם $h$ היא גזירה גם כן אז ה PDF של $Y$ באזור שבו $f_{Y}(y)>0$ תהיה

$$f_{Y}(y)= f_{X}(h(y)) \bigg|\frac{d_{h}}{d_{y}} (y)\bigg| $$
המשמעות של מה שבתוך הערך המוחלט היא שגוזרים את $h$ לפי המשתנה $y$ .

ההוכחה לכך נובעת מהעובדה ש 

$$F_{Y}(y)= P(g(X)\leq y)= P(X\leq h(y))= F_{X}(h(y))$$
קל לראות בתמונה למטה למה המעבר השני נכון

![Pasted image 20230126205419.png|450](/img/user/Assets/Pasted%20image%2020230126205419.png)

כעת אם נבצע גזירה על הנ״ל נקבל מכלל השרשרת 

$$f_{Y}(y)= f_{X}(h(y)) h^{\prime}(y)= f_{X}(h(y)) \bigg|\frac{d_{h}}{d_{y}} (y)\bigg| $$
הסיבה לערך המוחלט היא בגלל הטיפול במקרה שבו הפונקצייה מונוטונית יורדת ולכן הנגזרת היא אי חיובית אבל זה טיפול דומה למקרה להוכחה על הפונקצייה הליניארית, פשוט עובדים עם $1-F_{X}(h(y))$ ומבצעים על זה גזירה.

>[!info] 
>אם נרצה להסתכל על PDF במונחים של הסתברויות של אינטרוולים קטנים מאוד ששואפים ל0 אז הנוסחה הנ״ל נראת כבר יותר אינטואיטיבית 
> ![Pasted image 20230126211027.png|350](/img/user/Assets/Pasted%20image%2020230126211027.png)


## קונבולוצייה 
ישנם מקרים שבהם מפעילים פונקצייה $Z$ על שתי משתנים מקריים $X,Y$ , גם במצב זה התהליך של שתי השלבים שדיברנו עליו לא משתנה באופן משמעותי. 
אנחנו נגע במקרה פרטי של מצב כזה שהוא הסכום של שתי משתנים מקריים [[Computer Science/Probability/Independence on continuous random variables\|בלתי תלויים]] . כלומר במצב שבו 

$$Z=X+Y$$
עבור $X,Y$ בלתי תלויים.

### המקרה הבדיד
כדי לקבל תובנה יותר עמוקה של הסוגייה הזאת, נכנס קודם כל למקרה הבדיד כלומר נבין מה קורה ל PMF במצב הזה.

במקרה הבדיד המתאר את הנ״ל יתקיים 

$$\displaylines{
p_{Z}(z)= P(X+Y=z)\\ = \sum\limits_{(x,y) | x+y=z}P(X=x, Y=y)
\\ = \sum\limits_{x} P(X=x, Y=z-x) \\
= \sum\limits_{x} p_{X}(x)p_{Y}(z-x)
}$$
המעבר השני נובע בגלל איך שהגדרנו [[Computer Science/Probability/Joint PMF#פונקציות על מספר משתנים רנדומיים\|חישוב של פונקציות על מספר משתנים מקריים בדידים]] .

התוצאה הסופית שקיבלנו על $p_{Z}$ נקראת [קונבולוצייה](https://he.wikipedia.org/wiki/%D7%A7%D7%95%D7%A0%D7%91%D7%95%D7%9C%D7%95%D7%A6%D7%99%D7%94) של ה PMF של $X,Y$ .

![Pasted image 20230126212911.png|400](/img/user/Assets/Pasted%20image%2020230126212911.png)
ההסתברות  $p_{Z}(3)$ היא בעצם סכום כל הנקודות במרחב שסכומן הוא $3$ שאלה הנקודות שנמצאות למעלה. ההסתברות של נקודה כללית כזאת היא:
$$p_{X,Y}(x,3-x)= p_{X}(x)p_{Y}(3-x)$$

הקונבולוצייה מאפשרת לנו בעצם לקבל ערך בודד שמייצג את ההתפזרות של הנקודות הרצויות שלנו לכן זה גם נקרא קונבולוצייה- סוכמים את כל ההסתברויות שמקיימות נתון רצוי לכדי תוצאה בודדת שתמודל ב $Z$ כערך כלשהו. 

>[!info]
>דוגמה קלאסית לזה היא [בעיבוד תמונה](https://www.youtube.com/watch?v=B-M5q51U8SM) כאשר סוכמים קבוצת ערכים של פיקסלים לכדי ערך בודד  שמייצג מידע מסויים על הפיקסלים שסכמנו.

### המקרה הרציף
המקרה הרציף ייראה אומנם כתהליך מורכב יותר, אבל בסופו של דבר נרצה להגיע לאותה תוצאה, סכימה של נקודות שמקיימות תנאי מסויים, כלומר נרצה למצוא את ה PDF של $Z=X+Y$ , תיכף גם נראה שלמעשה נגיד לאותה התוצאה בידיוק פשוט בתצורה של אינטגרל ולא בתצורה של סכום.

אם כן עבור $X,Y$ משתנים רציפים בלתי תלויים יתקיים 

$$\displaylines{
P(Z\leq z|X=x) = P(X+Y\leq z | X=x) = P(x+Y\leq z | X=x) = P(x+Y\leq z)= P(Y\leq z-x)
}$$
המעבר השלישי נובע מ [[Computer Science/Probability/Independence on continuous random variables\|אי תלות של משתנים רציפים]]. נשים לב שזה אומר ש $F_{Z|\{X=x\}}(z|x)=F_{Y}(z-x)$ ולכן בביצוע אינטגרצייה על שני האגפים נקבל $f_{Z|X}(z|x)= f_{Y}(z-x)$ . נשתמש ב [[Computer Science/Probability/conditional probability#חוק הכפל\|חוק הכפל]] :


$$f_{X,Z}(x,z)= f_{X}(x)f_{Z|X}(z|x)= f_{X}(x)f_{Y}(z-x)$$
וסך הכל מהחילוץ של ה PMF השולי נקבל 

$$f_{Z}(z)= \int_{-\infty}^{\infty} f_{X,Z}(x,z)dx=\int_{-\infty}^{\infty}f_{X}(x)f_{Y}(z-x)dx$$

>[!info]
>דרך נוספת להוכיח את זה היא על ידי הנוסחה $f_{X+b}(x)= f_{X}(x-b)$ אבל לא אפרט את ההוכחה כאן שכן היא מאוד דומה לנ״ל. כמו כן מהמשפט הנ״ל משתמע שגם הפרש של משתנים מקריים מתנהג באופן דומה לחיבור שכן $X-Y$ יקיים ש $f_{-Y}(y)= f_{Y}(-y)$ כלומר אפשר להסתכל על $Z=X+(-Y)$ ולהשתמש במשפט כדי לקבל במקום $f_{-Y}(z-x)$ את $f_{Y}(x-z)$ 


שימו לב שזה מאוד דומה לנוסחה במקרה הבדיד פשוט מחליפים סכום באינטגרל ו [[Computer Science/Probability/Discrete Random Variables#PMF\|PMF]] ב [[Computer Science/Probability/Continuous Random Variables#PDF\|PDF]] . כיוון שהמשמעות היא די זהה, במקום לסכום מספר בדיד של נקודות סוכמים מספר אינסופי של נקודות שמקיים את זה.

![Pasted image 20230127000716.png|350](/img/user/Assets/Pasted%20image%2020230127000716.png)
מההבנה שההסתברות של הקו הזה היא המאורע $P(z\leq X+Y\leq z+\delta)\approx f_{Z}(z)\delta$ כפי שכבר ראינו בעבר שימוש בדומה במקומות אחרים. נוכל לחלץ את הנוסחה הנ״ל. 

__דוגמה__:
עבור $X,Y$ משתנים בלתי תלויים שמתפלגים באופן אחיד באינטרוול $[0,1]$ הסכום $Z=X+Y$ יהיה:

$$f_{Z}(z)= \int\limits_{-\infty}^{\infty} f_{X}(x)f_{Y}(z-x)dx$$
האינטגרנל $f_{X}(x)f_{Y}(z-x)$ הוא שונה מ$0$ ושווה ל $1$ עבור $0\leq x\leq 1 \ \ and \ \ 0\leq z-x\leq 1$ .על ידי קיבוץ של שתי אי השיוויונות האלה האינטגרנד יהיה שונה מ $0$ עבור

$$\max(0,z-1)\leq x\leq \min(1,z)$$
באמצעות המידע שיש לנו אנחנו יודעים להגיד ש $z\in[0+x,1+x]=[0,2]$ כלומר זה הטווח המקסימלי של ערכי $z$.

סך הכל יתקיים 

$$f_{Z}(z)= \begin{cases}\min(1,z)-\max(0,z-1) & z\in[0,2]\\ 0 & else\end{cases}$$
כאשר

$$\displaylines{\int\limits_{-\infty}^{\infty} f_{X}(x)f_{Y}(z-x)dx=\int\limits_{\max(0,z-1)}^{\min(1,z)}f_{X}(x)f_{Y}(z-x)dx=\int\limits_{\max(0,z-1)}^{\min(1,z)}1\cdot 1 dx= [x]^{\min(1,z)}_{\max(0,z-1)}\\=\min(1,z)-\max(0,z-1)} $$
הצורה המתקבלת היא המשולש הנ״ל
![Pasted image 20230127012058.png|400](/img/user/Assets/Pasted%20image%2020230127012058.png)
הגרף הזה בעצם מתאר לנו פונקצייה שהשטח שלה הוא בידיוק ״סכימת הנקודות״ שמקיימות את הדרוש והשטח זה ההסתברות של נקודה $(x,y)$ לקיים שסכומה הוא מספר ב $[0,2]$.

### סכום של שתי משתנים נורמליים
יהי $X,Y$ [[Computer Science/Probability/Normal Random Variable\|משתנים מתפלגים נורמלית]] בלתי תלויים עם תוחלת $\mu_{x},\mu_{y}$ ושונות $\sigma_{x}^{2},\sigma_{y}^{2}$ בהתאמה ויהי $Z=X+Y$ נקבל :

$$f_{Z}(z)= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi }\sigma_{x}}e ^{- \frac{(x-\mu_{x})^{2}}{2\sigma_{x}^{2}}}\cdot \frac{1}{\sqrt{2\pi}\sigma_{y}^{2}}e ^{- \frac{(z-x-\mu_{y})^{2}}{2\sigma_{y}^{2}}}dx$$
לא אפרט את הליך החישוב את התוצאה הסופית תהיה

$$f_{Z}(z)= \frac{1}{\sqrt{2\pi(\sigma^{2}_{x}+\sigma^{2}_{y}) }} e^{- \frac{(z-\mu_{x}-\mu_{y})^{2}}{2(\sigma_{x}^{2}+\sigma_{y}^{2})}}$$
מה שחשוב לשים לב כאן היא שהתוצאה היא משתנה מתפלג נורמלית עם תוחלת $\mu_{x}+\mu_{y}$ ושונות $\sigma_{x}^{2}+\sigma_{y}^{2}$ . כלומר __הסכום של שתי משתנים נורמלים הוא משתנה נורמלי בעצמו__ . מהמסקנה שהגענו עליה על הפעלת פונקצייה ליניארית על משתנה נורמלי נקבל סך הכל ש 

$$aX+bY$$

הוא גם כן משתנה נורמלי עבור $a\neq 0$ וסקלר $b$.

### חישוב גרפי של הקונבולוצייה 
נרצה להבין באופן גרפי מה קורה ב $f_{Z}(z)$ בהינתן קלט כלשהו $z$ ביחס לגרפים שממנה היא בנויה. אם כן נשתמש במשתנה סתמי $t$ כקלט של שתי הפונקציות ונסתכל על $f_{X}(t), f_{Y}(t)$ . עבור ערך $z$ אנחנו יודעים שהערך שלה בגרף יהיה.

$$f_{Z}(z)= \int\limits_{-\infty}^{\infty} f_{X}(t)f_{Y}(z-t)dt$$

החישוב הגרפי יבוצע בשלבים הבאים:

א) נסתכל על $f_{Y}(z-t)$ כפונקצייה של $t$ ונבנה לה גרף. הצורה שלה תהיה זהה כשל $f_{Y}(t)$ חוץ מהעובדה שהיא מתהפכת בציר ה x ואז היא עוברת הזזה לפי $z$. אם $z>0$ היא זזה לימין אחרת היא זזה לשמאל. 

ב) נשים את הפונקצייה הנ״ל ואת $f_{X}(t)$ אחד על השני ובונים את ההרכבה שלהם.

ג) מחשבים את הערך של $f_{Z}(z)$ על ידי חישוב האינטגרל של פונקציית ההרכבה משתי ההגרפים שבנינו.

![Pasted image 20230127115659.png|400](/img/user/Assets/Pasted%20image%2020230127115659.png)
_בהינתן ערך של $z$ כלשהו השטח שנמצא בשרטוט התחתון ביותר ייתן את ערך הפונקצייה $f_{Z}(z)$_

## שונות משותפת ומתאם
נרצה להשתמש בכלי מדיד שיאפשר לנו לאמוד את ה״עוצמה״ וה״כיוון״ של קשר בין שתי משתנים מקריים. 

השונות המשותפת של $X,Y$ מוגדרת להיות 

$$cov(X,Y)=E\big[(X-E[X])(Y-E[Y])\big] $$

כאשר $cov(X,Y)=0$ נומר ש $X,Y$ הם __בלתי משוייכים__. הסימן של ה cov עונה על השאלה האם $X-E[X]$ ו $Y-E[Y]$ הם עם אותו סימן או סימן הפוך. 

![Pasted image 20230127122045.png|450](/img/user/Assets/Pasted%20image%2020230127122045.png)
עבור $X,Y$  כמו בתמונה, שמתפלגים באופן אחיד באליפסות הנתונות (כל אליפסה מייצגת ניסוי אחר) ניתן לראות שבתמונה השמאלית $X$ הסטייה ממרכז המסה של $X,Y$ הם באותו כיוון כלומר ככל ש $X$ גדל מהתוחלת שלו כך גם $Y$ גדל מהתוחלת שלו, במצב זה ה שונות המשותפת תהיה גדולה מ0. במצב השני בתמונה הימנית זה הפוך, ככל ש $X$ גדל ממרכז המסה שלו שלו ככה $Y$ הולכים וקטנים ממרכז המסה  שלהם ולכן השונות המשותפת תהיה שלילית. הסיבה שההתפזרויות הנ״ל הן שליליות או חיוביות נובע ישירות מההגדרה שמבקשת את הכפל של מרחק של משתנה מסויים מהתוחלת כפול המרחק של השני ולכן האופן שבו הנקודות מתפזרות הוא קריטי.

נוסחה שקולה תהיה 

$$cov(X,Y)= E[XY]-E[X]E[Y]$$
_הוכחה:_

$$\displaylines{
E[(X-E[X])(Y-E[Y])]= E[XY-XE[Y]-YE[X]+E[Y]E[X]]\\= E[XY]-E[Y]E[X]-E[Y]E[X]+E[Y]E[X]= E[XY]-E[Y]E[X]
}$$


![Pasted image 20230129225130.png|300](/img/user/Assets/Pasted%20image%2020230129225130.png)
כאן ניתן לראות שגם פיזור הנקודות על גבי הרבעים משפיע על $E[XY]$ שיכול להשפיע על הסימן של התוחלת הנ״ל. הדוגמה זהה בתכליתה לתמונה למעלה רק שכאן זה מצב שבו התוחלת של $X$ או של $Y$ או של שניהם היא $0$ ואז מה שמשפיע על הסימן הוא הכפל בין ערכי הנקודות והפיזור שלהם לאורך הרבעים של הציר האנכי והאופקי..


### תכונות השונות המשותפת

$$\displaylines{
cov(X,Y)=cov(Y,X)\\
cov(X,X)= var(X) \\
cov(X,aY+b)= a\cdot cov(X,Y) \\
cov(X,Y+Z)= cov(X,Y)+cov(X,Z)\\
cov(X+Y,Z)= cov(X,Z)+cov(Y,Z) \\
cov(X,a)=0\\
cov(X,aX+b)=a\cdot var(X)
}$$

כמו כן בהינתן ש $X,Y$ [[Computer Science/Probability/Independence\|בלתי תלויים]] אנחנו יודעים ש $E[XY]=E[X]E[Y]$ ולכן יתקיים $cov(X,Y)=0$ . כלומר אם $X,Y$ בלתי תלויים הם גם בלתי משוייכים. נשים לב שההפך לא בהכרח נכון , נראה זאת בדוגמה:

הזוג של המשתנים המקריים $(X,Y)$ לוקח את הערכים $(1,0),(0,1),(-1,0),(0,-1)$ . כל אחד בהסתברות $\frac{1}{4}$ .

![Pasted image 20230127135920.png|200](/img/user/Assets/Pasted%20image%2020230127135920.png)

אם כן קל לראות שה [[Computer Science/Probability/Joint PMF\| PMF השולי]]  של $X,Y$ סימטרי סביב $0$ ולכן התוחלת של שתי המשנים היא $0$ . כמו כן נשים לב שלכל נקודה שנבחר יתקיים $XY=0$ ולכן $E[XY]=0$ , לכן 

$$cov(X,Y)= E[XY]-E[X]E[Y]=0$$
כלומר $X,Y$ הם בלתי משוייכים. עם זאת, $X,Y$ תלויים אחד בשני , אין סיבה כלל לבצע פה הליך חישובי כדי לראות זאת שכן 

$$P(X=0)= \frac{1}{2}$$

אבל 

$$P(X=0|Y=1)= 1$$

>[!info] התפזרות הערכים כאשר השונות המשותפת היא 0:
![Pasted image 20230127150330.png|200](/img/user/Assets/Pasted%20image%2020230127150330.png)


## מקדם המתאם של פירסון

נגדיר את מקסם המתאם $\rho(X,Y)$ של שתי משתנים מקריים עם שונות שונה מ$0$ על ידי:

$$\rho(X,Y)= \frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$$
זאת מעין גרסה מנורמלת של השונות המשותפת ולמעשה לא נוכיח זאת כאן אבל $\rho$ מקבל ערכים בין $-1,1$ .  ההתנהגות של מקדם המתאם כאשר הוא שונה מ0 זהה במשמעותה למה שהשונות המשותפת הייתה מראה לנו ההבדל הוא שהיא נותנת לנו מידע ללא תלות ביחידות מידע למשל אם $X,Y$ נמדדים במטרים אז השונות המשותפת נמדדת ב מטר מרובע ולכן זה יכול להפריע לתהליכים חישוביים שמשלבים בין השניים, לעומת זאת מקדם המתאם אינו תלוי ביחידות מידה , הוא יישאר במטרים.

באופן דומה נשמרת התכונה שאם המשתנים בלתי תלויים אז $\rho =0$ ולכן הם בלתי משוייכים .

אם כן, הגודל $|\rho|$ מספר לנו גרסה מנורמלת של השונות המשותפת בהינתן שאין לאף אחד מהמשתנים שונות ששווה ל0 (במצב זה המשתנה המקרי שקול לקבוע ואז מקדם המתאם אינו מוגדר היטב).

__משפט:__ 
$\rho=\pm 1$ אם ורק אם קיים קבוע $c$ כך ש 

$$Y-E[Y]= c(X-E[X])$$
המקרה הכי קל יהיה 

$$\rho(X,X)= cov(X,X )\cdot \frac{1}{var(X)}= \frac{var(X)}{var(X)}=1$$
וכמו ש 

$$X-E[X]= 1(X-E[X])$$

(באופן דומה על $\rho(X,-X)=-1$) 


__דוגמה__:
נניח שמטילים מטבע $n$ הטלות עם הסתברות ל head ששווה ל $p$ . יהי $X,Y$ מספר ה heads וה tails בהתאמה. נחפש את השונות המשותפת של $X,Y$ כאשר $X+Y=n$ וגם $E[X]+E[Y]=n$ (כי כל אחד מהם מתפלג גיאומטרית) .

דרך אחת להוכיח את זה:
$$E[X+Y]=E[n]=n  \ \ and \ \ E[X]+E[Y]=E[X+Y] $$
דרך נוספת: 

$$E[X]+E[Y]= np +n(1-p)= np+n-np =n$$

אם כן יתקיים מהעברת אגפים סטנדרטית :

$$X-E[X]= -(Y-E[Y])$$

כעת נחשב את השונות המשותפת של שניהם :

$$cov(X,Y)= E[(X-E[X])(Y-E[Y])]=-E[(X-E[X])^{2}]= -var(X)$$
לאחר נרמול נקבל כמובן $\rho(X,Y)=-1$ .


## השונות של סכום של משתנים מקריים
נוכל להשתמש בשונות המשותפת כדי לקבל נוסחה על השונות של סכום של מספר משתנים מקריים. 
נניח ש $X_{1},X_{2},\dots,X_{n}$ הם משתנים מקריים עם שונות סופית אזי :

$$var(X_{1}+X_{2})=var(X_{1})+var(X_{2})+2cov(X_{1}+X_{2})$$
_הוכחה:_

ויתקיים
$$\displaylines{
var(X+Y)= cov(X+Y,X+Y)= cov(X,X)+cov(X,Y)+cov(Y,X)+cov(Y,Y) \\= var(X)+2cov(X,Y)+cov(Y,Y)
}$$

ובאופן כללי יותר 

$$var\left(\sum\limits_{i=1}^{n} X_{i}\right)= \sum\limits_{i=1}^{n}var(X_{i})+\sum\limits_{\{(i,j) \ | \ i\neq j\}}cov(X_{i},X_{j})$$
_הוכחה:_

נסמן $\hat{X_{i}}= X_{i}-E[X_{i}]$ ויתקיים 

$$\displaylines{
var\left(\sum\limits_{i=1}^{n} {X_{i}}\right)= E\left[\left(\sum\limits_{i=1}^{n} \hat{X_{i}}\right)^{2}\right]= \\
E\left[\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n} \hat{X_{j}}\right] 
\\= \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n} E[\hat{X_{i}}\hat{X_{j}}] \\
=\sum\limits_{i=1}^{n}E[\hat{X_{i}^{2}}] + \sum\limits_{(i,j) \ | \ i\neq j} E[\hat{X_{i}}\hat{X_{j}}] 
\\ = \sum\limits_{i=1}^{n} var(X_{i})+ \sum\limits_{(i,j) \ | \ i\neq j} cov(X_{i},X_{j})
}$$

__דוגמה:__
נניח ש $n$ אנשים זורקים את הכובכים שלהם בקופסה ומוציאים את הכובע מהקופסא באקראי. נמצא את השונות של $X$ על ידי מידול של $n$ ניסויי ברנולי 

$$X=X_{1}+\dots + X_{n}$$
כאשר $X_{i}$ מקבל $1$ אם האדם ה $i$ הוציא את הכובע שלו בהסתברות $\frac{1}{n}$ ואחרת $0$.

נקבל אם כן 

$$E[X_{i}]= \frac{1}{n} \ \ var(X_{i})= \frac{1}{n}\left(1- \frac{1}{n}\right)$$
עבור $i\neq j$ נקבל 

$$\displaylines{
cov(X_{i}, X_{j})= E[X_{i},X_{j}]- E[X_{i}]E[X_{j}] \\
= P(X_{i}=1 \ and\ X_{j}=1)- \frac{1}{n^{2}}\\
= P(X_{i}=1)P(X_{j}=1 | X_{i}=1)- \frac{1}{n^{2}} \\
= \frac{1}{n}\cdot \frac{1}{n-1}- \frac{1}{n^{2}}\\= \frac{1}{n^{2}(n-1)}
}$$

וסך הכל: 

$$var(X)= var\left(\sum\limits_{i=1}^{n} X_{i}\right)= \sum\limits_{i=1}^{n}var(X_{i})+ \sum\limits_{\{(i,j) \ | \ i\neq j\}}cov(X_{i},X_{j}) $$

נציב את מה שקיבלנו 

$$n\cdot \frac{1}{n}\left( 1- \frac{1}{n}\right)+ n(n-1)\cdot \frac{1}{n^{2}(n-1)}= 1$$
