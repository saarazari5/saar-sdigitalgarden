---
{"dateCreated":"2023-01-27 17:41","tags":["probability","computer_science"],"pageDirection":"rtl","dg-publish":true,"permalink":"/computer-science/probability/limit-theorems/","dgPassFrontmatter":true}
---



# משפטי גבול

נרצה לדון ולהבין בבעיות מרכזיות שעוסקות בחסמים של סדרות של משתנים רציפים.
עיקר הדיון יעשה סביב רצף $X_{1},X_{2},X_{3}\dots$ של משתנים מקריים [[Computer Science/Probability/Independence\|בלתי תלויים]] שמתפלגים באופן זהה עם תוחלת $\mu$ ושונות $\sigma^{2}$ .

כעת, יהי

$$S_{n}= X_{1}+\dots + X_{n}$$
מליניאריות התוחלת אנחנו יודעים ש $E[S_{n}]=n\cdot\mu$ .

הסכום של $n$ המשתנים הראשונים. משפטי הגבול עוסקים בעיקר בתכונות של $S_{n}$ ומשתנים מקריים נוספים כאשר $n$ הוא מספר גדול מאוד. בגלל אי תלות נקבל 

$$var(S_{n})= var(X_{1})+var(X_{2})+\dots + var(X_{n})= n\sigma ^{2}$$

כלומר ככל ש $n$ גדל ההתפזרות של הערכים סביב התוחלת המשותפת גדלה ואין לה גבול שנותן לנו משמעות.  הסיטואציה משתנה אם מסתכלים על  [[Computer Science/Probability/Independence on discrete random variables#תוחלת ושונות של sample mean\|הערך הממוצע]] 

$$M_{n}= \frac{X_{1}+\dots+ X_{n}}{n}= \frac{S_{n}}{n}$$

אם נחשב אנחנו נראה ש 

$$E[M_{n}]= \mu \ \ \ var(M_{n})= \frac{\sigma^{2}}{n}$$
כלומר עבור $M_{n}$ מתקיים שהשונות שלה קרבה ל $0$ ככל ש $n$ גדל כלומר התפזרות הערכים יותר שואפת ל $\mu$ ככל שכמות הערכים שמוסיפים גדלה. אם כן הצלחנו ליצור משתנה מקרי $M_{n}$ שמתכנס לתוחלת האמיתית $\mu$ באופן די מדויק.
החוקים האלה מהווים בסיס מתמטי לפרשנות של התוחלת $E[X]=\mu$ כממוצע של מספר גדול מאוד של דגימות בלתי תלויות שנלקחות מההתפזרות של $X$ .

נסתכל גם על משתנה כמותי שמשמש כערך ביניים בין $S_{n}$ ל $M_{n}$ . ראשית נחסר מ $S_{n}$ את $n\cdot \mu$ כדי לקבל משתנה חדש עם תוחלת $0$ : $S-n\mu$ . לאחר מכן נחלק אותו ב $\sigma\sqrt{n}$ כדי לקבל את המשתנה המקרי 

$$Z_{n}= \frac{S_{n}- n\mu }{\sigma\sqrt{n}}$$
אפשר לראות שמתקיים 

$$E[Z_{n}]=0 \   \ \ \ \ var(Z_{n})=1$$

זה משתנה שערכו לא משתנה לא משנה כמה פעמים נגדיל את $n$ . __משפט הגבול המכרזי__ בודק ושואל על ההתפזרות של $Z_{n}$ ועל האופן שבו הוא חוסם ומבטיח שזוהי תהיה התפלגות נורמלית. 

>[!info]  משפטי הגבול עוזרים לנו במספר סיבות:
>1) קונספטואלית, הם נותנים לנו אינטרפטצייה לתוחלת (בנוסף להסתברויות) במונחים של רצף ארוך של משתנים מקריים זהים ובלתי תלויים , למשל לשאול המון המון אנשים את הגובה שלהם יכול להביא לנו את הגובה הממוצע באזור שבו שאלנו.
>2) הם מאפשרים לנו ניתוח מוערך של תכונות של משתנים מקריים כמו $S_{n}$ . זה בניגוד לניתוח מדויק שהיה דורש נוסחה של PMF ו PDF עבור $S_{n}$ שזו מטלה מאתגרת כאשר $n$ הוא גדול מאוד
>3) יש להם תפקיד מרכזי בניתוחים סטטיסטיים כאשר יש מספר גדול מאוד של מידע.

לפני שנכנס לכל החוקים שדיברנו עליהם למעלה, שנקרא גם החוק החלש של המספרים הגדולים (WLLN), נרצה לפתח מספר אי שיוויונות שיעזרו לנו לנתח הסתברויות קצה. האי שיוויונות שנציג יהיו יעילים כאשר נוכל לחשב את הערכים המדוייקים או החסמים של התוחלת והשונות של משתנה מקרי $X$ אבל  ההתפלגות של $X$ לא ברורה או קשה לחישוב.

## אי שוויון מרקוב (Markov inequality)
האי שיוויון הראשון שנתמקד בו הוא _אי שיוויון מרקוב_ באופן מופשט, הוא מבטיח ש אם משתנה מקרי אי שלילי הוא עם תוחלת מאוד קטנה, אז ההסתברות שיהיה לו ערך גבוה חייב להיות קטן.

יהי $X$ משתנה מקרי אי שלילי אזי :

$$P(X\geq a)\leq \frac{E[X]}{a} \ \ \ \ \text{for all a>0}$$
_הוכחה:_
יהי $a>0$ נגדיר משתנה מקרי $Y_{a}$ שמוגדר כך 

$$Y_{a}=\begin{cases} 0 & X<a \\ a & X\geq a\end{cases}$$
מההגדרה מתקיים ש $Y_{a}\leq X$ תמיד ולכן 

$$E[Y_{a}]\leq E[X]$$

מצד שני 

$$E[Y_{a}]= aP(Y_{a}=a)= aP(X\geq a)\leq E[X]$$

![Pasted image 20230127221516.png|450](/img/user/Assets/Pasted%20image%2020230127221516.png)
בתמונה ניתן לראות דוגמה לאי שיוויון זה. למעלה זה ה PDF של $X$ משתנה מקרי אי שלילי ולמטה ה PMF של $Y_{a}$ . ההרכבה של $Y_{a}$ נעשה על ידי הפרדה בין הצפיפות של ה $PDF$ של $X$ בין $0$ ל $a$ על ידי שיוך ל $0$ בפונקציית מסת ההסתברות, וכל מי שגדול או שווה ל $a$ בפונקציית הצפיפות משוייך ל $a$. כלומר אנחנו  מקטינים את המסה על ידי הזזה שמאלה ל $0$ או ל $a$ ולכן תמיד יתקיים 

$$E[X]\geq E[Y_{a}]= aP(Y_{a}=a)= aP(X\geq a)$$

__דוגמה__:
יהי $X$ משתנה מתפלג אחיד באינטרוול $[0,4]$ ונניח ש $E[X]=2$ . אי שיוויון מרקוב מבטיח ש 

$$\displaylines{
P(X\geq 2)\leq \frac{2}{2}=1 \\
P(X\geq 3)\leq \frac{2}{3}= 0.67 \\ 
P(X\geq 4)\leq \frac{2}{4}= 0.5
}$$

במקרה הזה אנחנו יודעים כיצד $X$ מתפלג ולכן נוכל לחשב ישירות ולאמת את האי שיווין

$$\displaylines{
P(X\geq 2)= 0.5 \\
P(X\geq 3)= 0.25 \\
P(X\geq 4) = 0
}$$
אם כן , האי שיווינים נכונים אבל הם יכולים להיות בטווחים גדולים מדי לעתים.

## אי שוויון צבישב (Chebyshev inequality)
אי שיוויון זה, באופן מופשט, מבטיח שאם משתנה מקרי הוא עם תוחלת נמוכה, אז ההסתברות שהוא ייקח ערך שרחוק מהתוחלת שלו היא גם כן נמוכה. אי שיוויון זה לא מצריך שהמשתנה המקרי יהיה אי שלילי

יהי $X$ משתנה מקרי עם תוחלת $\mu$ ושונות $\sigma^{2}$ אזי

$$P(|X-\mu|\geq c)\leq \frac{\sigma^{2}}{c^{2}} \ \ \ \ \text{for all c>0}$$
אפשר לראות שבאופן מתמטי זה הוא מקרה פרטי של אי שיוויון מרקוב. ובמילים המשמעות היא 
_אם השונות קטנה , אז $X$ באופן סביר לא יהיה רחוק מהתוחלת_ . שזה מסר קצת אחר מאי שיוויון מרקוב. 

_הוכחה:_
נגדיר משתנה מקרי $(X-\mu)^{2}$ ונפעיל את אי שיוויון מרקוב על $a=c^{2}$ , נקבל :

$$P((X-\mu)^{2}\geq c^{2})\leq \frac{E[(X-\mu)^{2}]}{c^{2}}= \frac{\sigma^{2}}{c^{2}}$$
האי שיוויון $(X-\mu)^{2}\geq c^{2}$ שקול למאורע $|X-\mu|\geq c$ ולכן נוכל פשוט להחליף את הנ״ל בהסתברות הדרושה. 

מקרה פרטי של אי שוויון צבישב יהיה מהצורה $c=k\sigma$ כאשר $k>0$ . זה יקיים 

$$P(|X-\mu|\geq k\sigma)\leq \frac{\sigma^{2}}{k^{2}\sigma^{2}}= \frac{1}{k^{2}}$$
כלומר ההסתברות שהמרחק של $X$ מהתוחלת להיות $k$ פעמים השונות שלו היא לכל היותר $\frac{1}{k^{2}}$ .

אי שיוויון צבישב נוטה להיות חזק יותר מאי שיוויון מרקוב (החסמים שהוא נותן מדוייקים יותר), בגלל שהוא משתמש במידע מהשונות של $X$. עדיין, השונות והתוחלת של משתנה הן רק חלק קטן מהמידע שיש לנו על משתנה מקרי ולכן אין לצפות לחסמים מדוייקים למדי.

### חסם עליון של אי שיוויון צבישב
כאשר $X$ נצמא בטווח ערכים $[a,b]$ נאמר ש 

$$\sigma^{2}\leq \frac{(b-a)^{2}}{4}$$ כלומר אם $\sigma^{2}$ לא יודע, נוכל להשתמש בחסם זה במקומו באי שיוויון צבישב ונקבל 

$$P(|X-\mu|\geq c)\leq \frac{(b-a)^{2}}{4c^{2}}$$

נוכיח את הטענה שהבאנו על החסם של $\sigma^{2}$ , ניקח $s$ קבוע כלשהו ונשים לב שמתקיים 

$$E[(X-s)^{2}]= E[X^{2}]-2E[X]s+s^{2}$$
אם $s=E[X]$ אז זה ערך מינימלי כיווון שנוכל להסתכל על הנ״ל כפונקצייה ריבועית של $s$ ונקבל
$$(s-E[X])^{2}+ E(X^{2})-E(X)^{2} $$
קל לראות שהמינימום של הפונקצייה הזאת מתקבל כאשר $(s-E[X])^{2}=0$ כלומר כאשר $s=E[X]$. אם כן, 
$$\sigma^{2}= E[(X-E[X])^{2}]\leq E[(X-s)^{2}]$$
אם נציב $s= \frac{a+b}{2}$ נקבל 

$$\sigma^{2}\leq E\left[\left(X- \frac{a+b}{2}\right)^{2}\right]= E[(X-a)(X-b)]+ \frac{(b-a)^{2}}{4}\leq \frac{(b-a)^{2}}{4}$$

המעבר האחרון נכון כי מהטווח ערכים של $x$ נקבל

$$(x-a)(x-b)\leq 0$$

### משלים של אי שיוויון צ׳בישב
נשים לב שבעזרת אי שיוויון צ׳בישב אפשר לחשב חסם תחתון להסתברות עבור מקרי האמצע בעזרת אותה נוסחה:

$$P(|X-E[X]|\leq c)= 1- P(|X-E[X]|\geq c)\geq 1- \frac{var(X)}{c^{2}}$$

_הוכחה:_

$$\displaylines{
P(|X-E[X]|\geq c)\leq  \frac{var(X)}{c^{2}} \\
-P(|X-E[X]|\geq c)\geq -  \frac{var(X)}{c^{2}} \\
1-P(|X-E[X]|\geq c)\geq 1-  \frac{var(X)}{c^{2}} \\ 
P(|X-E[X]|\leq c)\geq  1-  \frac{var(X)}{c^{2}} 
}$$


## החוק החלש של המספרים הגדולים
החוק החלש של המספרים הגדולים מבטיח שממוצע המדגם של מספר גבוה של משתנים בלתי תלויים שמתפלגים באופן זהה הוא מאוד קרוב לתוחלת בסבירות גבוהה. 
כפי שהראנו כבר בתחילת ההסבר 


$$M_{n}= \frac{X_{1}+\dots+ X_{n}}{n}= \frac{S_{n}}{n}$$

וגם

$$E[M_{n}]= \mu \ \ \ var(M_{n})= \frac{\sigma^{2}}{n}$$

אם נפעיל את אי שיוויון צבישב על $M_{n}$ נקבל

$$P(|M_{n}-\mu|\geq \epsilon)\leq \frac{\sigma^{2}}{n\epsilon^{2}}$$

נשים לב שלכל אפסילון חיובי צד ימי שואף ל $0$ ככל שמגדילים את $n$. ההשלכה של זה, נותנת לנו את החוק החלש של המספרים הגדולים. נשים לב שהדרישה היחידה שצריך עבור זה היא ש $E[X_{i}]$ יהיה מוגדר היטב

יהי $X_{1},X_{2}\dots$ סדרה של משתנים בלתי תלויים שמתפלגים באופן זהה עם תוחלת $\mu$ . אזי לכל $\epsilon>0$ נקבל

$$P(|M_{n}-\mu|\geq\epsilon)= P\left(\left| \frac{X_{1}+\dots+ X_{n}}{n} -\mu\right|\geq \epsilon\right)\overset{n\to\infty}{\to}0$$

המשמעות של זה היא שלא משנה כמה גדול $n$ יהיה וכמה קטן $\epsilon$ יהיה ההתפלגות של $M_{n}$ מרוכזת סביב התוחלת $\mu$ . בעצם זה אומר שלכל אינטרוול $[\mu-\epsilon,\mu+\epsilon]$ מסביב ל$\mu$ , אזי יש הסתברות גבוהה ש $M_{n}$  יתפלג בתוך האינטרוול הזה וככל ש $n\to\infty$ ההסתברות הזאת הולכת וקרבה ל $1$. 
_כמובן שאם אפסילון קטן יותר ככה נצטרך יותר ערכי $n$ כדי להגיע לתוצאה הזאת, אבל עדיין זה מובטח שזה יקרה מתישהו_.

>[!info] שימוש של חוק זה:
>אם נבצע ניסוי שמורכב מניסויים קטנים $X_{i}=\mu+W_{i}$  כאשר $W_{i}$ זוהי איזשהי הפרעה עם תוחלת של $0$ כלומר $E[W_{i}]=0$ . הרעיון הוא שלמרות ההפרעות האלה עדיין נקבל ש $M_{n}$ , ממוצע המדגם, ייתן בסבירות גבוהה שמרחקו יהיה קרוב ל $\mu$ . כלומר נוכל לעשות ניסויים חוזרים ורבים של אותו ניסוי ולמדל את $M_{n}$ ונוכל לקבל בקירוב טוב את התוחלת 

__הסתברויות של תדרים__:
נניח מאורע $A$ שמוגדר בניסוי הסתברותי כלשהו ונסמן $p=P(A)$ .
נבנה $n$ חזרות בלתי תלויות של אותו הניסוי ונגדיר $M_{n}$ להיות הממוצע של מספר הפעמים ש $A$ התרחש בסדרת הניסויים האלה , במצב זה מגדירים את $M_{n}$ להיות [empirical frequency](https://en.wikipedia.org/wiki/Empirical_probability) של $A$ . אם כן,

$$M_{n}= \frac{X_{1}+\dots+X_{n}}{n}$$
כאשר $X_{i}$ הוא משתנה אינדיקטור שמקבל $1$ כאשר $A$ קרה ו $0$ אם לא. יתקיים,  $E[X_{i}]=p$ .  המשמעות היא שככל ש $n$ גבוה יותר ככה $M_{n}$ שואף להיות בסביבה אפסילונית של $p$ . זה אומר שיש קשר הדוק בין כמות הניסויים שנעשה לבין ההסתברות להצליח בניסוי. כלומר נוכל לפרש את ההסתברות $p$ כ״תדירות ההתרחשות של מאורע $A$״.

> [!note] חוק החזק של המספרים הגדולים
> ישנו חוק נוסף שלא אפרט עליו כאן שנקרא, החוק החזק של המספרים הגדולים שמהדק את החוק החלש על ידי כך שהוא אומר שההסתברות ש$M_{n}$ יהיה שווה ל $\mu$ כאשר $n\to\infty$ היא __שווה__ ל $1$. כלומר
> $$P(\lim_{n\to\infty} M_{n}=\mu) =1$$ 

__polling - דגימות__:
יהי $p$ שמייצג את החלק של המצביעים שתומכים שמועמד מסויים לרשות המשרד. אנחנו מראיינים $n$ מצביעים ״שנבחרו באקראיות״ ובודקים את $M_{n}$ ואת החלק ממנו שתומך במועמד הנ״ל. נרצה להסתכל על $M_{n}$ כהערכה של $p$ ונרצה לחקור את תכונותיו. נשים לב ש $n$ המצביעין שבחרנו נבחרים באופן בלתי תלוי ואחיד ביחס לשאר האוכלוסייה. כלומר נוכל למדל כל בן אדם שעונה האם הוא מצביע למועמד או לא כמשתנה ברנולי  בלתי תלוי $X_{i}$ עם הסתברות להצלחה של $p$ ושונות $\sigma^{2}=p(1-p)$ .
אי שיוויון צבישב מעיד על כך ש 

$$P(|M_{n}-p|\geq \epsilon)\leq \frac{p(1-p)}{n\epsilon^{2}}$$
אומנם אנחנו לא יודעים מיהו $p$ אבל אנחנו יודעים שלכל $X_{i}$ טווח הערכים הוא בין $0$ ל $1$ ולכן כפי שכבר אמרנו נוכל לחסום :

$$p(1-p)\leq \frac{1}{4}$$

כלומר

$$P(|M_{n}-p|\geq \epsilon)\leq \frac{1}{4n\epsilon^{2}}$$

כך למשל יתקיים שבהצבת $\epsilon=0.1$ ו $n=100$ יתקיים

$$P(|M_{100}-p|\geq 0.1)\leq \frac{1}{4\cdot 100\cdot 0.01}= 0.25$$
כלומר עבור דגימה של 100 אנשים ההסתברות שההערכה שלנו לא נכונה ביותר מ $0.1$ היא לא יותר גבוהה מ$\frac{1}{4}$ . זה כבר נותן לנו מידע די אמין ומקורב להסתברות שהמועמד ייבחר , שכן בהתסברות של $0.75$ שההערכה שלנו כן תהיה בדיוק של $0.1$ סביב ההסתברות. נוכל להדק את ההסתברות להיות לפחות $0.95$ שהערכה שלנו תהיה נכונה בסביבה אפסילונית של $0.1$ סביב $p$ . אם כן 

$$P(|M_{n}-p|\geq 0.01)\leq \frac{1}{4n(0.01)^{2}}$$
נרצה שיתקיים 

$$\frac{1}{4n(0.01)^{2}}\leq 1-0.95=0.05$$
אם נחשב נקבל שהדרישה היא ל $n\geq 50000$ . הבחירה של $n$ הזאת תספק אותנו אבל עדיין מוטלת בספק בגלל שהיא מובססת על אי שיוויון צבישב שהוא לא הדוק במיוחד..

## משפט הגבול המרכזי
בתחילת ההסבר הגדרנו את 

$$S_{n}= X_{1}+\dots + X_{n}=nM_{n}$$
כמובן שמתקיים 

$$E[S_{n}]= nE[M_{n}]=n\mu$$

נראה כי בניגוד ל$M_{n}$ , שבה השונות שואפת ל$0$ , כאן השונות 

$$var(S_{n})= var(X_{1})+var(X_{2})+\dots + var(X_{n})= n\sigma ^{2}$$
שואפת ל$\infty$ ביחס ל$n$ . ביצענו נורמליזצייה והזזה של המשתנה הזה כדי לקבל משתנה עם תוחלת $0$ ושונות $1$. נזכר:

$$Z_{n}= \frac{S_{n}-n\mu}{\sigma\sqrt{n}}= \frac{X_{1}+\dots+X_{n}-n\mu}{\sigma\sqrt{n}}$$

נקבל 

$$\displaylines{
E[Z_{n}]= \frac{E[X_{1}+\dots+X_{n}]-n\mu}{\sigma\sqrt{n}}=0 \\ \\
var(Z_{n})= \frac{var(X_{1}+\dots+X_{n})}{\sigma^{2}n}= \frac{var(X_{1})+\dots+ var(X_{n})}{\sigma^{2}n}= \frac{n\sigma^{2}}{n\sigma^{2}}=1
}$$

__נגדיר את משפט הגבול המרכזי באופן הבא:__ 
יהי $X_{1},X_{2},\dots$ סדרה של משתנים בלתי תלויים וזהים בהתפלגותם עם תוחלת $\mu$ ושונות $\sigma^{2}$ אזי ה$CDF$ של $Z_{n}$ שואף ל $CDF$ של [[Computer Science/Probability/Normal Random Variable#משתנה נורמלי סטנדרטי\|המשתנה הנורמלי הסטנדרטי]] 

$$\phi(z)= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{z}e^{\frac{-x^{2}}{2}}dx$$
כלומר 

$$\lim_{n\to\infty}P(Z_{n}\leq z)= \phi(z)$$

המשפט הזה חזק מאוד, כי הוא כללי מאוד. חוץ מאי תלות והעובדה שהתוחלת והשונות מוגדרים היטב וסופיים, __אין שום דרישות נוספות על המשתנים האלו__. הם יכולים להיות בדידים, רציפים, מעורבים וכו.
למשפט זה יש חשיבות רבה ממספר סיבות , גם תיאורתית וגם פרקטית. בצד התיאורתי , זה מעיד על כך שהסכום של מספר רב של משתנים בלתי תלויים הוא בקירוב מתפלג נורמלית וניתן להשתמש בזה בכל מצב שבו יש אפקט מקרי כלשהו שהוא הסכום של מספר גבוה של גורמים בלתי תלויים. דוגמה טובה לזה היא מידוש של רעשים והפרעות במערכות מיחשוב ובעוד מגוון רחב של תרחישים נוספים. באופן כללי, הסטטיסטיקה של רעשין והפרעות הן ממודלות היטב על ידי התפלגות נורמלית בידיוק מהסיבה הזאת.
בצד הפרקטי, משפט הגבול המרכזי מוריד את הצורך במודלים הסתברותיים מפורטים ובמניפולציות מורכבות על פונקציות הצפיפות או המסה. כל שצריך לעשות הוא לגשת לטבלת ה CDF המוכרת של ההתפלגות הנורמלית.

### קירובים מבוססים משפט הגבול המרכזי
משפט הגבול המרכזי מאפשר לנו לחשב הסתברויות שקשורות ל $Z_{n}$ כאילו הוא היה נורמלי. 
כיוון שנורמליות נשמרת תחת טרנספורמציות ליניאריות נוכל באופן שקול להסתכל על $S_{n}$ כמשתנה נורמלי עם תוחלת $n\mu$ ושונות $n\sigma^{2}$ 

__משפט קירוב נורמלי מבוסס משפט הגבול המרכזי__:
יהי $S_{n}=X_{1}+X_{2}+\dots+X_{n}$ נוכל להתייחס להסתברות $P(S_{n}\leq c)$ בקירוב נורמלי באופן הבא:

1. חשב את $n\mu$ והשונות $n\sigma^{2}$ של $S_{n}$ .
2. חשב את הערך המנורמל $z=\frac{c-n\mu}{\sigma\sqrt{n}}$  .
3. תשתמש בקירוב: 

$$P(S\leq c)\approx \Phi(z)$$

__דוגמה 1:__
טוענים למטוס 100 חבילות שמשקלם הוא משתנה מקרי בלתי תלוי שמתפלג באחידות בין 5 ל 50 קילוגרם.
מהי ההסתברות שהמשקל הכולל של החבילות יעבור את 3000 קילוגרם?
זאת לא תהיה משימה קלה לחשב אתה CDF של כל המשקלים וההסתברות הרצויה, אבל נוכל להשיג קירוב טוב על ידי משפט הגבול המרכזי. נרצה אם כן, לחשב

$$S_{100}>3000$$

כאשר $S_{100}$ זה משקל של 100 חבילות. התוחלת והשונות של כל חבילה מחושבת לפי התוחלת והשונות של משתנה אחיד 

$$\mu = \frac{5+50}{2}=27.5 \ \ \ \ \sigma^{2}= \frac{(50-5)^{2}}{12}= 168.75$$

כעת נחשב את הערך המנורמל 

$$z= \frac{3000-100\cdot27.5}{\sqrt{168.75\cdot 100}}= 1.92 $$ 
כעת נוכל להשתמש בCDF של משתנה מקרי נורמלי 

$$P(S_{100}\leq 3000)\approx \Phi(1.92)= 0.9726$$
וההסתברות הרצויה תהיה

$$P(S_{100}\geq 3000)= 1-P(S_{100}\leq 3000)\approx 1- 0.9726= 0.0274$$

__דוגמה 2:__
מכונה מעבדת חלקים של מידע , חלק אחד כל פרק זמן. זמן העיבוד של החלקים השוני הם בלתי תלויים שמתפלגים באיחודים ב $[1,5]$. 
נרצה לאמוד את ההסתברות  שמספר החלקים המעובדים ב320 יחידות זמן יהיה לכל הפחות $100$.
באותו אופן, נשאף לחשב את $N_{320}$ , יהיה קשה לחשב אותו כסכום של משתנים בלתי תלויים אבל נוכל להסתכל על הבעיה מזוויצ אחרת של סכימת פיסות המידע המעובד. אם כן כמו מקודם , נחשב את $S_{100}$ שזה זמן העיבוד של 100 חתיכות המידע הראשונות, ונחליף את המאורע $\{N_{320}\geq 100\}$ במאורע השקול 
$\{S_{100}\leq 320\}$ , כעת נוכל להשתמש בקירוב הנורמלי כמו מקודים כאשר התוחלת של כל $X_{i}$ היא $3$ והשונות היא $\frac{4}{3}$ . יתקיים

$$z= \frac{320-n\mu}{\sigma\sqrt{n}}= \frac{320-300}{\sqrt{100\cdot \frac{4}{3}}}=1.73$$

כעת נוכל לחשב את הקירוב:

$$P(S_{100}\leq 320)\approx \Phi(1.73)= 0.9582$$

__polling__: 
נחזור לבעייתה הדגימות ממקודם. אנחנו מעוניינים בהתסברות 

$$P(|M_{n}-p|\geq \epsilon)$$
בגלל הסימטריה של התפלגות נורמלית סביב התוחלת אנחנו יודעים שאפשר להוריד את הערך המוחלט אם פשוט נכפיל פעמיים את ההסתברות בלי הערך המוחלט כלומר

$$P(|M_{n}-p|\geq\epsilon)\approx 2P(M_{n}-p\geq\epsilon)$$

נזכיר של $M_{n}$ יש תוחלת $p$ ושונות של $p(1-p)$ . נשים לב שבמצב זה יש לנו בעיה שהשונות לא ידועה לנו כי היא תלויה ב $p$. אם כן, ניקח חסם עליון כלשהו על $M_{n}-p$ באמצעות חסם העליון על התוחלת שראינו כבר ולכן הוא יהיה $\frac{1}{4n}$ הסיבה בכלל שלקחנו חסם עליון היא שראינו שההסתברות הכי גדולה שהתפזרות הערכים תהיה רחוקה מהתוחלת באה לידי ביטוי כשהשונות היא הגדולה ביותר אז לקחנו את המקרה הגרוע אבל נראה שזה ייתן קירוב טוב יותר משימוש באי שיוויון צבישב. כמו כן, אם נציב בנוסחה נקבל שזה יקרה כש$p=\frac{1}{2}$ .

אם כן נחשב את הערך המנורמל 

$$z= \frac{\epsilon}{\frac{1}{2\sqrt{n}}}$$

ובהצבה נקבל

$$P(M_{n}-p\geq\epsilon)\leq 1-\Phi(z)= 1-\Phi(2\epsilon\sqrt{n})$$

כלומר יש לנו תלות במספר האנשים שאנחנו דוגמים. למשל אם נשאל $100$ אנשים ונרצה סביבה אפסילונית של $0.1$ כמו בדוגמה הקודמת אזי

$$\displaylines{
P(|M_{100}-p|\geq 0.1)\approx 2P(M_{n}-p\geq 0.1)\leq 2-2\Phi (2\cdot0.1\cdot \sqrt{100})\\=2-2\Phi(2)= 2-2\cdot 0.977=0.046
}$$

זה הרבה יותר קטן ומדויק מההערכה של $0.25$ שקיבלנו על ידי שימוש באי שיוויון צבישב.

__כמו כן נשים לב שגם חישוב הבעיה ההפוכה הופך להיות פשוט, אם נרצה לדעת מה $n$ צריך להיות כדי לקבל את $p$ בקירוב של $0.95$ פשוט מציבים את הגורמים הרלוונטים במשוואה ומחשבים__

>[!warning] 
>הקירוב הנ״ל הוא מדוייק ככל ש $n$ שואף ל $\infty$ אבל בפועל אנחנו תמיד עובדים עם $n$ סופי, והתשובה למהו ה$n$ הטוב ביותר שנוכל לקחת משתנה בין התפלגויות שונות ובעיקר כמה הההתפלגות הזאת קרובה להתפלגות נורמלית או שהיא סימטרית. למשל, עבור התפלגות אחידה כבר $S_{8}$ יתאים לנו וייתן קירוב מדויק מאוד לנורמלי. אבל אם $X_{i}$ הוא מעריכי נצטרך $n$ גדול בהרבה. 
>למשל עבור המשתנה הבדיד שמתפלג אחיד הבא
>![Pasted image 20230128181215.png|350](/img/user/Assets/Pasted%20image%2020230128181215.png)
>נקבל שעבור $n=2$ כלומר הקונבולוצייה עם עצמו תיתן
>![Pasted image 20230128181323.png|350](/img/user/Assets/Pasted%20image%2020230128181323.png)
> וככל שנגדיל את $n$ הקונבולוצייה תקבל צורה שיותר קרובה לגרף הפעמון המוכר מהנורמלי 
> ![Pasted image 20230128181511.png|500](/img/user/Assets/Pasted%20image%2020230128181511.png)



### קירוב נורמלי של משתנה בינומי
משתנה מקרי בינומי $S_{n}$ עם פרמטר $n$ ו $p$ הוא בעצם הסכום של $n$ משתני ברנולי בלתי תלויים $X_{1}\dots X_{n}$ עם פרמטר משותף $p$. נזכר ש 

$$\mu= E[X_{i}]=p \ \ \ \sigma=\sqrt{var(X_{i})}=\sqrt{p(1-p)}$$

נראה מה קורה כאשר משתמשים בקירוב שמשפט הגבול המרכזי נותן לנו כדי לספק קירוב טוב של ההסתברות למאורע $\{k\leq S_{n}\leq l\}$ , כאשר $k,l$ הם מספרים כלשהם. אם כן אם ננרמל כפי שעשינו עד כה 

$$k\leq S_{n}\leq l \leftrightarrow \frac{k-np}{\sqrt{np(1-p)}}\leq \frac{S_{n}-np}{\sqrt{np(1-p)}}\leq \frac{l-np}{\sqrt{np(1-p)}}$$

לפי משפט הגבול המרכזי נקבל ש $\frac{S_{n}-np}{\sqrt{np(1-p)}}$ מקורב להפלגות נורמלית ולכן 

$$\displaylines{P(k\leq S_{n}\leq l)= P\left(\frac{k-np}{\sqrt{np(1-p)}}\leq \frac{S_{n}-np}{\sqrt{np(1-p)}}\leq \frac{l-np}{\sqrt{np(1-p)}}\right)\\\approx\Phi\left(\frac{l-np}{\sqrt{np(1-p)}}\right)-\Phi\left(\frac{k-np}{\sqrt{np(1-p)}}\right)}$$

הקירוב הזה שקול ללהתייחס ל $S_{n}$ כמשתנה מתפלג נורמלית עם תוחלת $np$ ושונות $np(1-p)$ 

![Pasted image 20230128191331.png](/img/user/Assets/Pasted%20image%2020230128191331.png)

התמונה למעלה מראה לנו PMF של התפלגות בינומית עם הPDF הנורמלי. אפשר לראות שהקירוב של מציאת ההסתברות בא לידי ביטוי על ידי אינטגרצייה של השטח מתחת לPDF הנורמלי , בטווח שבין $k$ ל $l$ שזה השטח האפור בתמונה. מצב בעייתי יכול להיות במקרה זה שאם $k=l$ נקבל שההסתברות היא $0$ ולכן תיקון של זה יהיה להשתמש בטווח $k- \frac{1}{2}$ ל $k+ \frac{1}{2}$ כדי לקבל קירוב של $P(S_{n}=k)$ . בתמונה הימנית אפשר לראות הרחבה של הרעיון הזה באופן כללי שלוקח את הטווח $[k- \frac{1}{2},l+ \frac{1}{2}]$ כדי שיהיה אפשר לקבל קירוב טוב של הסתברות של כמו במקרה למעלה.

>[!info] 
>הקירוב הטוב ביותר יהיה כאשר $p\approx \frac{1}{2}$ ובמצב זה ההתפלגות הבינומית הי די סימטרית, אפשר במצב זה להגיע לקירוב מצויין כאשר $n$ יהיה באזור ה $40$ עד $50$. ככל שההסתברות מתחזקת לכיוון אחד הערכים הקיצוניים, כלומר $1$ או $0$ ככה איכות הקירוב יורדת

#### De Moivre-Laplace Approximation to the Binomial
אם כן אחרי שרשמנו באופן מופשט נגדיר את הקירוב הנ״ל. אם $S_{n}$ הוא משתנה המתפלג בינומית אזי 

$$P(k\leq S_{n}\leq l)\approx\Phi\left(\frac{l+ \frac{1}{2}-np}{\sqrt{np(1-p)}}\right)-\Phi\left(\frac{k- \frac{1}{2}-np}{\sqrt{np(1-p)}}\right) $$

__דוגמה:__ 
יהי $S_{n}$ משתנה המתפלג בינומית עם פרמטר $n=36$ ו $p=0.5$ החישוב המדוייק יהיה

$$P(S_{n}\leq 21)= \sum\limits_{k=0}^{21}\binom{36}{k}(0.5)^{36}= 0.8785$$
אם נחשב את הקירוב לפי התפלגות נורמלית נקבל 

$$P(S_{n}\leq 21)\approx \Phi\left(\frac{21-np}{\sqrt{np(1-p)}}\right)= \Phi\left(\frac{21-18}{3}\right)= \Phi(1)= 0.8413$$

אם נחשב לפי הקירוב שאמרנו למעלה (להוריד את הטווח בחצי)

$$P(S_{n}\leq 21)\approx \Phi\left(\frac{21.5-np}{\sqrt{np(1-p)}}\right)=\Phi\left(\frac{21.5-18}{3}\right)= \Phi(1.17)=0.879$$

ניתן לראות שזה קירוב הרבה יותר טוב והוא גם מאפשר לנו לחשב לאמוד הסתברות לערך יחיד למשל

$$P(S_{n}=19)\approx\Phi\left( \frac{19.5-18}{3}\right)-\Phi\left( \frac{18.5-18}{3}\right)= 0.6815-0.5675=0.124$$
וזה מאוד קרוב לערך המדוייק 

$$\binom{36}{19}(0.5)^{36}= 0.1251$$

