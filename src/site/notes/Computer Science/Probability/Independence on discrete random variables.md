---
{"dateCreated":"2022-12-09 19:51","tags":["probability","computer_science"],"pageDirection":"rtl","dg-publish":true,"permalink":"/computer-science/probability/independence-on-discrete-random-variables/","dgPassFrontmatter":true}
---



# אי תלות על משתנה רנדומי בדיד

נדבר על הקשר בין [[Computer Science/Probability/Independence\|אי תלות]] לבין משתנים רנדומיים.

## אי תלות של משתנה רנדומי עם מאורע
אי תלות של משתנה רנדומי עם מאורע זה מצב דומה לאי תלות של שתי מאורעות. הרעיון הוא שהידיעה שהתרחשוב של מאורע כלשהו לא מוסיפה כלל מידע על הערך של המשתנה הרנדומי. באופן פורמלי יותר נוכל להגיד ש $X$ הוא __בלתי תלוי__ במאורע $A$ אם יתקיים
$$P(X=x \cap A) = P(X=x)P(A)= p_{X}(x)P(A)$$
כל עוד $P(A)>0$ , אי תלות היא זהה לתנאי 
$$\forall_{x}: p_{X|A}(x)= p_{X}(x)$$
__דוגמה__:
נסתכל על ניסוי של שתי הטלות מטבע הוגן בלתי תלויות. נגדיר $X$ להיות מספר הפעמים שיצא $H$ ונגדיר את המאורע $A$ להיות המאורע שבו מספר הפעמים שיצא $H$ הוא זוגי. פונקציית מסת ההסתברות של $X$ תהיה
$$p_{X}(x)= \begin{cases} \frac{1}{4}& x=0\\ \frac{1}{2}&x=1\\ \frac{1}{4}& x=2\end{cases}$$
כעת אנחנו יודעים ש $P(A)= \frac{1}{2}$ ומכאן נוכל לבנות את פונקציית מסת ההסתברות המותנת במאורע
$$p_{X|A}(x)= \begin{cases} \frac{1}{2}& x=0\\ 0&x=1\\ \frac{1}{2}& x=2\end{cases} $$

קל לראות אם כן ש $X$ ו $A$ תלויים כן פונקציית מסת ההסתברות המותנת והלא מותנת אינן שוות.

## אי תלות בין משתנים רנדומים
נאמר ששתי משתנים רנדומיים $X,Y$ יהיו בלתי תלויים אם 
$$\forall_{x,y}:p_{X,Y}(x,y)=p_{X}(x)p_{Y}(y)$$
באופן אינטואיטיבי זה אומר שהערכים של $Y$ לא נותנים מידע כלל על הערכים של $X$.

## אי תלות מותנת
נוכל להגדיר אי תלות מותנת על שתי משתנים רנדומיים $X,Y$ בהינתן מאורע $A$ עם הסתברות גדולה מ0. 
המאורע המתנה $A$ מגדיר יקום חדש וכל ההסתברויות ופונקציות מסת ההסתברות למינהן צריכות להתחלף על ידי המקבילות המותנות שלהן.
נאמר ש $X,Y$ הן אי תלויות באופן מותנה אם עבור $A$ הנ״ל יתקיים
$$\forall_{x,y}: P(X=x,Y=y | A)= P(X=x|A)P(Y=y|A)$$
שזה שקול ללרשום 
$$\forall_{x,y}: p_{X,Y|A}(x,y)= p_{X|A}(x)p_{Y|A}(y)$$
ונוכל להרחיק לכת ולנסח זאת גם 
$$\forall_{x,y\text{ such that: } p_{Y|A}(y)>0}: p_{X,Y|A}(x,y)= p_{X|A}(x)$$

__נשים לב שאין בהכרח קשר בין אי תלות מותנת לאי תלות שאינה מותנת__ לדוגמה בתמונה הבאה 

![Pasted image 20221210143252.png|350](/img/user/Assets/Pasted%20image%2020221210143252.png)
המשתנים $X,Y$ עצמם אינם תלויים למשל 
$$p_{X|Y}(1|1)= P(X=1|Y=1)=0\neq P(X=1)=p_{X}(1)= \frac{3}{20}$$
עם זאת, אם נגדיר את המאורע $A= \{X\leq 2, Y\geq 3\}$ נקבל אי תלות בהינתן המאורע הזה.

## תוחלת ואי תלות
נאמר שאם $X,Y$ הם בלתי תלויים אז 
$$E[g(X)h(Y)]=E[g(X)]E[h(Y)]$$
לכל פונקצייה $g,h$ .
נוכל להוכיח זאת באופן הבא:
נגדיר $T(X,Y)=g(X)h(Y)$ ויתקיים
$$\displaylines{
E[T(X,Y)]= \sum\limits_{x}\sum\limits_{y}g(x)h(y)p_{X,Y}(x,y)=\sum\limits_{x}\sum\limits_{y}g(x)h(y)p_{X}(x)p_{Y}(y) \\
=\sum\limits_{x}g(x)p_{X}(x)\sum\limits_{y}h(y)p_{Y}(y)= E[g(x)]E[h(y)]
}$$
__נשים לב שזה מתקיים גם אם $g,h = I$ כלומר פונקציית הזהות ולכן $X,Y \text{ is independent}\rightarrow{ E[XY]=E[X]E[Y]}$


## שונות ואי תלות
נסתכל על הסכום $X+Y$ של שתי משתנים אקראיים בלתי תלויים. בגלל שהשונות לא משתנה על אם נוסיף סקלר ממשי על המשתנה הרנדומי נוכל להוכיח טענה על השונות עם משתני __אפס תוחלת__ שיוגדרו כך
$$X^{\prime}= X-E[X]  \ \ , \ \  Y^{\prime}=Y-E[Y]$$
נשים לב שכל פעולה ליניארית על שתי המשתנים האלה תשאיר את התוחלת 0 ובפרט המשתנה הרדנומי $X^{\prime}+Y^{\prime}$ גם הוא יהיה עם תוחלת 0. הרבה יותר קל לעבוד במצב כזה שכן חלק מהביטוי מתבטל כליל .  

לא בעיה להוכיח מליניאריות התוחלת שזה יוצא תוחלת אפס לשתי המשתנים. אם כן:
$$\displaylines{
var(X+Y)= var(X^{\prime}+Y^{\prime})= E[(X^{\prime}+Y^{\prime})^{2}]= E[X^{2\prime }+2X^{\prime}Y^{\prime}+ Y^{\prime2}]= \\
E[X^{2^{\prime}}]+ 2E[X^{\prime}Y^{\prime}]+E[Y^{2^{\prime}}]=
\\ E[X^{2^{\prime}}]+ 2(E[X^{\prime}]E[Y^{\prime}])+ E[Y^{2^{\prime}}]= E[X^{2^{\prime}}]+E[Y^{2^{\prime}}] = \\
var(X^{\prime})+var(Y^{\prime})= var(X)+var(Y)
}$$


העובדה ש $2E[X^{\prime}Y^{\prime}]=2(E[X^{\prime}]E[Y^{\prime}])=0$ נובעת מאי תלות והעובדה שאלו משתני תוחלת 0.

אם כן לסיכום יקיים שאם $X,Y$ בלתי תלויים אז
$$var(X+Y)=var(X)+var(Y)$$
## אי תלות של מספר משתנים רנדומים
נרצה להכליל אי תלות למצב שבו יש יותר משתי משתנים רנדומיים, למשל עבור $X,Y,Z$ נאמר שהם בלתי תלויים אם:
$$\forall_{x,y,z}: p_{X,Y,Z}(x,y,z)= p_{X}(x)p_{Y}(y)p_{Z}(z)$$
נשים לב לעובדה מעניינת, 
אם המשתנים הנ״ל בלתי תלויים אז גם כל $f(X),g(Y),h(Z)$ שנבחר יהיו בלתי תלויים. באופן דומה , כל 2 משתנים מקריים מהצורה $g(X,Y),h(Z)$ הם בלתי תלויים.
__למרות זאת__ ברוב המקרים שתי פונקציות $g(X,Y),h(Y,Z)$ כן יהיו תלויות אחד בשני בגלל ששתיהן מושפעות מ $Y$ . 

### השונות של הסכום של n משתנים רנדומיים
סכום של משתנים בלתי תלויים חשובים לנו במספר דרכים, למשל, הם באים לידי ביטוי ביישומים סטטיסטיים שבהם נרצה לעשות ממוצע של מספר מדידות בלתי תלויות תוך כדי שאיפה למזער את האפקט של שגיאות במדידות שלנו. גם הם באים לידי ביטוי כשמתמודדים עם האפקט המצטבר של מספר מקורות רנדומליים בלתי תלויים. 

__משפט__ אם $X_{1},\dots,X_{n}$ הם משתנים בלתי תלויים אזי 
$$var\left(\sum\limits_{i=1}^{n} X_{i}\right)= \sum\limits_{i=1}^{n} var(X_{i})$$
ההוכחה שלה היא באינדוקצייה כאשר הבסיס הוא על שתי משתנים כפי שכבר הוכחנו.

## השונות של משתנה בינומי
ניקח $n$ הטלות מטבע בלתי תלויות , כאשר כל הטלה היא בהסתברות $p$ לצאת $H$ .
נוכל להגדיר להטלה ה$i$ משתנה ברנולי אינדיקטור במחזיר $1$ אם יצא $H$ ו$0$ אחרת,  ויתקיים 
$$X=X_{1}+X_{2}+X_{3}+\dots+ X_{n}$$
כאשר $X$ הוא מספר הפעמים שיצא $H$ . 
נזכיר שהתוחלת של משתנה זה היא $E[X]=np$. אם כן בגלל שההטלות הן בלתי תלויות ולכן כל משתנה ברנולי הוא בלתי תלוי בקודמו יתקיים
$$var(X)=var\left(\sum\limits_{i=1}^{n} X_{i}\right)= \sum\limits_{i=1}^{n}var(X_{i}) = np(1-p)$$
## השונות של משתנה פואסון
נזכיר שמשתנה פואסון $Y$ עם פרמטר $\lambda$ הוא דומה ל״גבול״ של משתנה בינומי כאשר $n\to\infty$ ו $p\to 0$ כאשר $\lambda = np$ . אם כן , באופן לא פורמלי עבור ההתכנסות של $n,p$ כפי שציינו למעלה נקבל ש 
$$var(Y)=E[Y]=np=\lambda$$
נוכיח על השונות באופן הבא
$$\displaylines{
E[Y^{2}]= \sum\limits_{k=1}^{\infty} \frac{k^{2}e^{-\lambda}\lambda^{k}}{k!}= \lambda\sum\limits_{k=1}^{\infty} \frac{k^{}e^{-\lambda}\lambda^{k-1}}{(k-1)!} =\\
\lambda\sum\limits_{m=0}^{\infty} (m+1) \frac{e^{-\lambda}\lambda^{m}}{m!}= \lambda\left(\sum\limits_{m=0}^{\infty} \frac{e^{-\lambda}\lambda^{m}}{(m-1)!} + \sum\limits_{m=0}^{\infty} \frac{e^{-\lambda}\lambda^{m}}{m!}\right)= \\
\lambda(E[Y]+1)= \lambda(\lambda+1)
}$$

מכאן נוכל לחלץ 
$$var(Y)= E[Y^{2}]-(E[Y])^{2}=\lambda(\lambda+1)-\lambda^{2}=\lambda$$


## תוחלת ושונות של sample mean 
נרצה לאמוד את שיעורי ההצבעה של הנשיא ביידן בבחירות האמצע , נסמנו $B$ . שאלנו $n$ אנשים שנלקחו באקראי משיעורי ההצבעה ונגדיר את $X_{i}$ להיות המשתנה הרנדומי שקובע האם האדם ה $i$ הצביע לביידן או לא (משתנה אינדיקטור).
נסמן את התוחלת של $X_{i}$ להיות $p$ כהסתברות שהוא הצביע ביידן.

נגדיר את ה sample mean $S_{n}$ להיות 
$$S_{n}= \frac{\sum\limits_{i=1}^{n}X_{i}}{n}$$

זהו משתנה רנדומי שמייצג את שיעורי ההצבעה של ביידן במרחב המדגם שהוא $n$ האנשים.
מליניאריות התוחלת נקבל
$$E[S_{n}]= \sum\limits_{i=1}^{n} \frac{1}{n}E[X_{i}]= \frac{1}{n}np=p$$
מהשונות של משתנים בלתי תלויים נקבל 
$$var(S_{n})= \frac{1}{n^{2}}\sum\limits_{i=1}^{n}var(X_{i})= \frac{p(1-p)}{n}$$

נשים לב שזה משתנה רנדומי שמהווה אסטימצייה טובה לשיעורי ההצבעה בגלל שהתוחלת שלו היא עדיין $p$ שמייצג את שיעור ההצבעה ״האמיתי״ של ביידן. נשים לב שכל עוד $X_{i}$ בלתי תלויים אחד בשני ויש להם תוחלת ושונות שווים אז יתקיים 
$$var(S_{n})= \frac{var(X)}{n}$$
