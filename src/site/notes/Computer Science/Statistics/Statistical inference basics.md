---
{"dateCreated":"2023-08-02 16:01","tags":["statistics","cs_biu"],"pageDirection":"rtl","dg-publish":true,"permalink":"/computer-science/statistics/statistical-inference-basics/","dgPassFrontmatter":true}
---



# מבוא לסטטיסטיקה היסקית
כפי שדיברנו ב [[Computer Science/Statistics/Descriptive statistics\|סטטיסטיקה תיאורית]] , אנחנו אוספים מידע על מדגם מתוך אוכלוסייה. כיוון שהמידע הוא רק על מדגם ולא על האוכלוסייה כולה, צריך לבצע הסקת מסקנות כלשהי באמצעות כלים הסתברותיים.
בעצם סטטיסטיקה היסקית היא הקשר בין תורת ההסתברות לבין תהליך ההסקה.

__דגימה__ , __אוכלוסייה__ , __מדגם__ הם שלושת המרכיבים העיקריים של כל בעיית הסקה.
כאשר המדגם נעשה בצורה __מקרית__ יש בידינו כלים מתמטיים שמאפשרים לנו לקבוע את מידת הדיוק של אומדן כזה.

## דגימה מקרית
דגימה מקרית של מדגם בגודל $n$ מאוכלוסייה בגודל $N$ היא בחירה של $n$ איברים מתוך האוכלוסייה כך שלכל איבר באוכלוסייה יש הסתברות שווה להיכלל במדגם.

_לדוגמה_: פיקוד העורף מעוניין לבדוק את כשירותם של המקלטים באזור גוש דן. 
האוכלוסייה תהיה כלל המקלטים בגוש דן ומתוכם יבחר פיקוד העורף מדגם יחסית קטן של 200 מקלטים בשיטת דגימה מקרית.

_דוגמה נוספת:_ משרד התחבורה מעוניין לבדוק מהו אחוז כלי הרכב הבלתי תקינים הנעים בכבישי הארץ ולכן הוא עורף בדיקות פתח ל $1000$ מכוניות שנבחרות באופן מקרי בכבישים שונים.

זאת דוגמה יחסית פשוטה. נסתכל על דוגמה מעט מבלבלת יותר, 
מכון התקנים מעוניין לבדוק את איכות נורות החשמל המיוצרות במפעל מסויים, כלומר מה אחזור הנורות התקינות היוצאות מקו הייצור של המפעל. 

נשאלת כאן השאלה, מי היא בכלל האוכלוסייה שכן מכון התקנים מעוניין להעריך את איכות הנורות שיווצרו בעתיד בנוסף לאלה שכבר נוצרו. 

ניעזר ב [[Computer Science/Probability/Discrete Random Variables\|משתנה מקרי]] ופונקציות ההסתברויות שלו, כדי לענות על השאלה הזאת.
אם כן, נוכל להגדיר משתנה מקרי אינדיקטור $X$ ונרצה לבדוק מהי ההסתברות $p=P(X=1)$ . 
בדיקה שכזו נקראת __תצפית__ מתוך המ״מ $X$.
כמובן שתצפית אחת לא תספיק ולכן יש צורך לערוך תצפיות רבות 

$$X_{1},X_{2},\dots, X_{n}$$

כאשר $X_{i}$ מסמן את התצפית ה$i$.
כלומר קיבלנו כאן סדרה של משתני אינדיקטור בלתי תלויים עם אותה פונקציית הסתברות ולכן 

$$P(X_{i}=x_{i})=\prod_{i=1}^{n}P(X_{i}=x_{i})$$

כלומר לקחנו סדרת תצפיות בלתי תלויות שהניבו ערך כלשהו בהסתברות כלשהי וביצענו חישוב מתמטי על ההסתברות של כל אחת מהתצפיות שנתנה לנו את ההסתברות הכוללת. 

>[!note] הגדרה
>__מדגם מקרי בגודל $n$ מתוך מ״מ $X$__ הוא סדרה של $n$ משתנים מקריים (תצפיות) $X_{1},\dots,X_{n}$ , בלתי תלויים, שלכל אחד מהם פונקציית הסתברות השווה לפונקציית ההסתברות של $X$

__משפט__
דגימה מקרית עם החזרה של $n$ איברים מתוך אותה אוכלוסייה שקולה למדגם מקרי בגודל $n$ מתוך משתנה מקרי מתאים.

נסתכל למשל על הדוגמה הראשונה עם המכוניות 
נסמן את התקינות של כלי רכב כמשתנה מקרי אינדיקטור $T$ .
נשים לב שמתקיים 

$$P(T=0)= \frac{N(0)}{N}$$

כאשר $N$ זה מספר כלי הרכב בישראל ו $N(0)$ זה מספר כלי הרכב הלא תקינים.
באותו אופן :

$$P(T=1)= \frac{N(1)}{N}$$

וכמובן ש $N(1)=N-N(0)$ ולכן פונקציית ההסתברות היא

$$P(T=i)= \begin{cases}
 1- \frac{N(0)}{N}&i= 1\\ \frac{N(0)}{N} & i=0 
\end{cases}$$

כלומר היחס בין כלי הרכב הבלתי תקינים לבין כלי הרכב בישראל שקולה לפונקציית ההסתברות הרצויה. 
אם כן מה שיקרה בניסוי הוא :
נדגום רכב כלשהו-->נחזיר אותו לאוכלוסיית כלי הרכב-->נדגום כלי רכב נוסף
כל דגימה תהיה $T_{i}$ כאשר הדגימה האחרונה תהיה $T_{1000}$ .
כיוון שמחזירים את הרכב לאוכלוסייה הרי שפונקציית ההסתברות שקולה בין כל דגימה והם בלתי תלויים זה לזה ולכן קיבלנו את השקילות שרצינו.

>[!note] הערה
>ההחזרה נועדה להבטיח שהאוכלוסייה הנדגמת לא תשתנה מדגימה לדגימה. אי החזרת איבר משנה את האוכלוסייה ואז התצפיות הופכות להיות תלויות.
>רק כאשר הדגימה קטנה מאוד ביחס לאוכלוסייה ניתן להזניח את ההבדל הזה.

>[!note] הבחנה
>גם דגימה מקרית מתוך מ״מ $X$ היא בעצם דגימה מקרית מתוך אוכלוסייה. שכן ההסתברות מתארת באיזשהו מקום __שכיחות יחסית__ ולכן תמיד נוכל לתאר משתנה מקרי עם פונקציית הסתברות כאוכלוסייה עם $N$ כדורים וכל ערך בפונקציית ההסתברות, הסתברותו תומר ביחס $p\cdot N$ כאשר $p$ זו ההסתברות המתאימה לערך $x$ כלשהו בפונקציית ההסתברות

## התפלגות דגימה
השאלה העיקרית לאחר שהבנו מהו מדגם מקרי תהיה 
__מה וכיצד ניתן ללמוד מהמדגם על האוכלוסייה או על המ״מ שממנו נלקח?__ 

נדגיש כי תהליך הדגימה הוא ניסוי מקרי וכל אחת מהתוצאות $X_{i}$ היא משתנה מקרי.
המשמעות היא, חזרות על תהליך הדגימה יניבו מדגמים שונים.
נרצה להסיק מתוך מדד כלשהו במדגם כל מה שניתן על המדד המתאים באוכלוסוייה.

>[!info] הגדרה
>למדד __באוכלוסייה__ או במ״מ קוראים __פרמטר__
הפרמטר הוא גודל קבוע המאפיין את האוכלוסייה או את התפלגות המ״מ.

ה[[Computer Science/Probability/Discrete Random Variables#תוחלת\|תוחלת]] למשל היא פרמטר של משתנה מקרי $X$ בידיוק כמו שאחוז כלי הרכב הבלתי תקינים הוא פרמטר של אוכלוסיית כלי הרכב.

נעזר במה שנקרא __התפלגות דגימה__ כדי ללמוד ממדדי המדגם (הסטטיסטים) על תכונות האוכלוסייה (הפרמטרים).

אם כן, התפלגות הדגימה של סטטיסטי מסויים (סטטיסטי יכול להיות שונות של מדגם, ממוצע של מדגם וכו׳) היא פונקציית ההסתברות שלו. התפלגות זו תלויה בצורתו המתמטית של הסטטיסטי, בגודל המדגם ובתכונות האוכלוסייה שממנה נדגם.

נגדיר מספר מונחים:
* __התפלגות אוכלוסייה__: הקשר בין ערכי משתנה מסויים $X$ לבין שכיחותו באוכלוסייה או שכיחותו היחסית. המדדים בהתפלגות זו מכונים _פרמטרים._
* __פונקציית ההסתברות של משתנה מקרי $X$:__ הקשר בין ערכים אפשריים של $X$ לבין ההסתברות לקבלתם.
* __התפלגות המדגם:__ הקשר בין ערכי משתנה מסויים $X$ לבין שכיחותו במדגם או שכיחותו היחסית. אלו מכונים _סטטיסטים_. 
* __התפלגות דגימה:__ הקשר בין הערכים האפשריים של סטטיסטי מסוים לבין ההסתברות לקבלתם.

__במה תלויה צורת ההתפלגות הדגימה?__
__א.__ בהתפלגות האוכלוסייה שדוגמים ממנה.
__ב.__ בגודל המדגם. למשל אם המדגם הוא בגודל 1 אז התפלגותו תהיה זהה להתפלגות האוכלוסייה ולמדגמים בגודל כל האוכלוסייה אין כלל התפלגות דגימה.
__ג.__ בסוג הסטטיסטי שאותו מחשבים במדגמים השונים.

לכן מקפידים לומר:
_״התפלגות הדגימה של סטטיסטי מסוים, למדגמים בגודל $n$ שנלקחו מאוכלוסייה מתוארת מסוימת״_

==נסתכל על דוגמה:== 
יש לנו אוכלוסייה של $10000$ תלמידים והתפלגות הציונים כי כך
![Screenshot 2023-08-02 at 18.15.13.png|400](/img/user/Assets/Screenshot%202023-08-02%20at%2018.15.13.png)
זוהי __ההתפלגות באוכלוסייה__ של מ״מ $X$ המייצג את הציון באוכלוסייה. __ההתפלגות ההסתברות__ של מ״מ $X$ נקבעת על ידי השכיחות היחסית של כל ציון.

אם נדגום מהאוכלוסייה $10$ תלמידים בדימה מקרית נקבל למשל את הנתונים הללו:

![Screenshot 2023-08-02 at 18.19.02.png|250](/img/user/Assets/Screenshot%202023-08-02%20at%2018.19.02.png) 
זוהי __התפלגות__ השכיחויות __במדגם__ , ואף עבורה אפשר לחשב מדדים למשל $\overline{x}=6.9$ .

אם נסתכל על מדגם בגודל $2$ נקבל שהממוצע יכול לקבל רק אחד מהערכים 

$$5,5.5,6,6.5,7,7.5,8,8.5,9$$
מהתפלגות האוכלוסייה אפשר לראות שבמדגם בין שני תלמידים הממוצע $9$ יהיה פחות סביר מהממוצע $7$.
ישנם דרכים לחשב את ההסתברות של כל ממוצע אפשרי במדגמים הללו בגודל $2$ מתוך האוכלוסייה שתוארה. מה שנקבל הוא __התפלגות הדגימה של הממוצע__ למדגמים בגודל $2$ .
![Screenshot 2023-08-02 at 18.22.21.png](/img/user/Assets/Screenshot%202023-08-02%20at%2018.22.21.png)

==דוגמה 2:==
התוצאות של הטלת קובייה תקינה הן מ״מ $X$ שפונקציית ההסתברות שלו היא
![Pasted image 20230802224701.png|350](/img/user/Assets/Pasted%20image%2020230802224701.png)
דגימה בגודל $n$ מהתפלגות זו פירושה הטלת הקובייה $n$ פעמים. 
נניח שהקובייה מוטלה פעמיים ונתעניין בסטטיסטי שהוא ממוצע התוצאות של שתי ההטלות.

נגדיר $n=2$ ונרשום בה את ממוצעי התוצאות האפשריות של כל שתי הטלות
![Screenshot 2023-08-02 at 22.51.20.png](/img/user/Assets/Screenshot%202023-08-02%20at%2022.51.20.png)
ההסתברות לקבל כל אחד מזוגות אלו היא זהה, כיוון שיש $36$ זוגות, ההסתברות לקבלת כל זוג היא $\frac{1}{36}$ . לפיכך אנו יכולים לרשום את ההסתברות של כל ממוצע ולקבל את ההתפלגות של הדגימה $\overline{X}$ למדגמים בגודל $2$.
![Screenshot 2023-08-02 at 22.52.49.png](/img/user/Assets/Screenshot%202023-08-02%20at%2022.52.49.png)
אם כן, __זוהי התפלגות הדגימה של $\overline{X}$ עבור מדגמים בגודל 2 שנלקחו מהתפלגות התוצאות של קובייה תקינה__

נסתכל על התוחלת והשונות של המ״מ של התפלגות הדגימה הנ״ל
![Screenshot 2023-08-02 at 23.18.06.png](/img/user/Assets/Screenshot%202023-08-02%20at%2023.18.06.png)
תחילה חישבנו את התוחלת של התפלגות הדגימה. לשם כך הכפלנו כל ממוצע בהסתברותו, קיבלנו מכך את הטור השלישי. סכום הטור השלישי מבטא את התוחלת, על פי הנוסחה שאנחנו מכירים 

$$E[\overline{X}]=\sum\limits_{\overline{x}}\overline{x}P(\overline{x})=3.5$$
נוכל גם לחשב את השונות של המ״מ של התפלגות הדגימה עם הטבלה הנ״ל

$$\sigma^{2}_\overline{x} = E\left[\overline{X} - E(\overline{X})\right]^{2}= \sum\limits P(\overline{x})[\overline{x}-E(\overline{X})]^{2}$$

## התפלגות הדגימה של הממוצע
כאמור, ממוצע המדגם $\overline{X}$ הוא מ״מ בעל פונקציית הסתברות. פונקציה זו תלויה בגודל המדגם ובמ״מ שממנו נלקח. למרות תלות זו אפשר לציין תכונות כלליות מסוימות של פונקציית ההסתברות של ממוצע המדגם. 
__תוחלת__
$$E[\overline{X}]= E\left( \frac{X_{1}+X_{2}+\dots+X_{n}}{n}\right)= E\left(\sum\limits_{i=1}^{n} \frac{1}{n}X_{i}\right)= \frac{1}{n}\sum\limits_{i=1}^{n}E(X_{i})$$

נזכיר שלכל $X_{i}$ יש פונקציית הסתברות זהה לזו של המשתנה $X$ שעליו נערכות התצפיות. ולכן $E(X_{i})= E(X)$ כלומר מתקיים ש 

$$E(\overline{X})= \frac{1}{n}\sum\limits_{i=1}^{n}E(X)= nE(X)\cdot \frac{1}{n}= E(X)$$
ובמילים: __תוחלת הסטטיסטי ״ממוצע המדגם״ שווה לתוחלת המ״מ שממנו דוגמים.__ 

אם $\overline{X}$ הוא מ״מ המתאים להתפלגות של גודל מסוים, $X$, באוכלוסייה נתונה (גובה, גיל, משקל וכו׳), הרי $E(\overline{X})$ הוא בידיוק ממוצע האוכלוסייה והשוויון הנ״ל אומר ש __בדגימה מקרית מאוכלוסייה, תוחלת ממוצע המדגם שווה לממוצע באוכלוסייה__.
אם נחשב את התוחלת של דוגמת הקוביות ממקודם נקבל $E(\overline{X})=3.5$ שזה בידיוק התוחלת של הטלה בודדת $E(X)$ , שזה גם בידיוק הממוצע של ההטלות השונות  $\frac{1+2+3+4+5+6}{6}=3.5$ .

נשים לב, במדגמים שונים מאותה אוכלוסייה יכולים להתקבל ממוצעי מדגם שונים, אך אם נדגום הרבה מאוד מדגמים ובכל אחד מהם נחשב את הממוצע, אזי ממוצע הממוצעים יהיה קרוב מאוד לממוצע האוכלוסייה. 
אנחנו רוצים להבין אם כן, מהי הסבירות שממוצע המדגם שלנו סטה בהרבה מממוצע האוכלוסייה. 
מכיוון שממוצע האוכלוסייה זהה לממוצע הממוצעים $(E(\overline{X}))$ אז אנחנו שואלים מהי הסבירות שממוצע המדגם שלנו יהיה רחוק מהתוחלת שלו שהיא $E(\overline{X})$ .

אם כן, שואלים על __מידת הפיזור__ של התפלגות הדגימה של הממוצע.
[[Computer Science/Statistics/Descriptive statistics#מדדי פיזור\|מדד הפיזור]] המקובל ביותר הוא השונות ולכן נחשב את השונות על התפלגות הדגימה של הממוצע

$$\sigma^{2}_\overline{X} = V(\overline{X})= V\left[\frac{1}{n}\sum\limits_{i=1}^{n}X_{i}\right]= \frac{1}{n^{2}}V\left(\sum\limits_{i=1}^{n}X_{i}\right)$$
המעבר האחרון נובע מכך ש $V(aX)=a^{2}V(X)$

אנחנו יודעים ש [[Computer Science/Probability/Further Topics on Random Variables#השונות של סכום של משתנים מקריים\|השונות של סכום משתנים בלתי תלויים שווה לסכום של השונויות שלהם]] .  כיוון שכל $X_{i}$ בלתי תלויים אחד בשני ובפרט הם מקיים שיש להם את אותה פונקציית הסתברות ותוחלת למשתנה המקרי שממנו הם נדגמו  אזי גם את אותה השונות ולכן 

$$V(\overline{X})= \frac{1}{n^{2}}[nV(X)]= \frac{V(X)}{n}$$

ובעצם נקבל סך הכל ש 

$$\sigma^{2}_\overline{X} = \frac{1}{{n}}\sigma^{2}_{X}\to \sigma_{\overline{X}}= \frac{1}{\sqrt{n}}\sigma^{}_{X} $$

ובמילים: __סטיית התקן של ממוצע המדגם שווה לסטיית התקן של המ״מ שממנו דגמנו, מחולקת בשורש הרובעי של גודל המדגם__.

המסקנה העיקרית היא שכככל שהמדגם גדול יותר כך שונות ממוצע המדגם קטנה יותר, ושעל ידי בחירת מדגם גדול נוכל להקטין שונות זו כרצוננו. 
בתצורה גרפית ניתן לראות שפונקציית הצפיפיות של $\overline{X}$ תהיה מרוכזת יותר ויותר בקרבת הממוצע ככל שגודל המדגם עולה. 

למשל עבור $n_{3}>n_{2}>n_{1}$ :
![Screenshot 2023-08-02 at 23.36.31.png|450](/img/user/Assets/Screenshot%202023-08-02%20at%2023.36.31.png)

לגודל סטיית התקן יש השפעה ישירה על ההסתברות שהמשתנה המקרי יקבל ערכים רחוקים מהתוחלת. למשל ההסתברות שממוצע המדגם $\overline{X}$ יקבל ערך גדול או שווה ל $a$ , $P(\overline{X}\geq a)$ , שווה לשטח המקווקו מתחת לפונקציית הצפיפות שבתרשים הנ״ל.
הסתברות זו הולכת וקטנה ככל שסטיית התקן קטנה. כלומר ממוצע המדגם בתרשים השלישי יהיה בסבירות גבוה הרבה יותר קרוב לתוחלת $E(\overline{X})$

המסקנה הזו מכונה [[Computer Science/Probability/Limit Theorems#החוק החלש של המספרים הגדולים\|חוק המספרים הגדולים]] וביטויה הכמותי מבוסס על מה שמכונה [[Computer Science/Probability/Limit Theorems#אי שוויון צבישב (Chebyshev inequality)\|אי שיוויון צ׳בישב]] .

## אי-שיוויון צ׳בישב

לכל משתנה מקרי $X$ בעל תוחלת $\mu$ ושונות $\sigma^{2}$ ולכל מספר חיובי $k$ מתקיים:

$$P(\mu-k\sigma<X<\mu+k\sigma)\geq 1- \frac{1}{k^{2}}$$
ביטוי שקול יכול להיות:
$$P(|X-\mu|<k\sigma)\geq 1- \frac{1}{k^{2}}$$
והמשלים:

$$P(|X-\mu|\geq k\sigma)\leq \frac{1}{k^{2}}$$

למשל עבור $k= 2$ נקבל $P(X\in(\mu-2\sigma,\mu+2\sigma))\geq 1- \frac{1}{4}= \frac{3}{4}$ ומשמעות הדבר היא שבהתסברות $\frac{3}{4}$ לפחות יקבל $X$ ערך המרוחק מן התוחלת $\mu$ בפחות משתי סטיות תקן. 
![Screenshot 2023-08-03 at 0.58.55.png|350](/img/user/Assets/Screenshot%202023-08-03%20at%200.58.55.png)

ככל ש $k$ גדול יותר הסביבה סביב התוחלת גדול יותר. כלומר הרווח מתרחב וההסתברות להיות בשטח הזה גדלה יותר. הגדולה של אי שיוויון צ׳בישב היא בכך שישנה הערכה כמותית לקצב הגידול של ההסתברות זו עם גידול $k$ של: הסתברות זו היא לפחות $1- \frac{1}{k^{2}}$ .

__דוגמה:__
התפלגות מנות המשכל של סטודנטים הלומדים בבר אילן היא בעלת ממוצע $\mu =120$ וסטית תקן $\sigma=8$ . נשתמש באש״צ כדי למצוא תחום שבו נמצאות מנות המשכל של לפחות $\frac{3}{4}$ מהסטודנטים. נסמן ב $Y$ את מנת המשכל של סטודנט הנבחר באופן מקרי. $Y$ הוא מ״מ בעל תוחלת $\mu=120$ וסטיית תקן $\sigma=8$. אנו מעוניינים בתחום שעבורו קיים $1- \frac{1}{k^{2}}= \frac{3}{4}$ כלומר $k=2$ 
אם נציב $k=2$ נקבל 

$$P(Y\in(104,136))\geq \frac{3}{4}$$

כלומר מנת המשכל של לפחות $\frac{3}{4}$ מהסטודנטים נמצאת בתחום שבין $104$ ל $136$. 

>[!note] הבחנה
>אי שיוויון צ׳בישב נכון לכל מ״מ בעל שונות סופית, ללא כל קשר לפונקציית ההסתברות שלו. מכאן נובע יתרונו הגדול- אפשר להפעילו מבלי לדעת את פונקציית ההסתברות שלו. מכאן נובע יתרונו הגדול- אפשר להפעילו מבלי לדעת את פונקציית ההסתברות. נשים לב שבגלל שהוא כל כך כללי הוא לא מפורט במידה מספקת. למשל, הוא לא נותן לנו הערכה משמעותית לגבי ההסתברות בקירוב של פחות מסטיית תקן אחת.

כעת, נפעיל את אי שיוויון צ׳בישב כדי ללמוד על ההתנהגות של __ממוצע המדגם__ 

$$\overline{X}= \frac{1}{n}(X_{1}+\dots+ X_{n})$$

מהכלים שלמדנו על ההתנהגות של השונות והתוחלת של ממוצע המדגם נקבל כי 

$$P\left(\mu - k\cdot \frac{\sigma}{\sqrt{n}}< \overline{X}< \mu+k\cdot \frac{\sigma}{\sqrt{n}}\right)\geq 1- \frac{1}{k^{2}}$$

## חוק המספרים הגדולים
נשים לב שאת $k$ נוכל לבחור כרצוננו.
עבור כל גודל חיובי קטן כרצוננו, $\varepsilon$ , נבחר את $k$ כך $k\geq\varepsilon \frac{\sqrt{n}}{\sigma}$ . נציב בנוסחה למעלה ונקבל 

$$P(\mu-\varepsilon< \overline{X}< \mu+\varepsilon)\geq 1- \frac{\sigma^{2}}{\varepsilon^{2}n}$$

כעת נוכל לקרב את גודל האגף בימין ל $1$ כרצוננו על ידי הגדלת $n$ שזה גודל המדגם. כלומר, נוכל לגרום לכך שבהסתברות קרובה ל $1$ יימצא ממוצע המדגם קרוב ל $\mu$ עד כדי $\varepsilon$ לכל היותר. 
חוק זה נקרא __חוק המספרים הגדולים__ שמבטיח לנו שעל ידי בחירת מדגם גדול כל צורכו נוכל להיות כמעט בטוחים שהממוצע שלו יהיה קרוב מאוד לממוצע האולכוסייה.

באופן מתמטי מדוייק יותר מתקיים 

$$\lim_{n\to\infty} P(\mu-\varepsilon<\overline{X}<\mu+\varepsilon)\geq 1$$

אבל הסתברות היא תמיד קטנה מ$1$ ולכן 

$$\lim_{n\to\infty} P(\mu-\varepsilon<\overline{X}<\mu+\varepsilon)\leq 1$$

וסך הכל מכללי אי שיוויון מתקיים 

$$\lim_{n\to\infty} P(\mu-\varepsilon<\overline{X}<\mu+\varepsilon)= 1$$

__מסקנה חשובה__
נתבונן בניסוי מקרי כלשהו, יהי $A$ מאורע הקשור בו בעל הסתברות $p$ וערכו $0$ אם תוצאת הניסוי אינה ב $A$ בהסתברות $1-p$ כלומר נוכל למדל את פונקציית ההסתברות באמצעות משתנה אינדיקטור $X$. התוחלת שלו היא $p$ ושונותו היא $p(1-p)$ .

נסתכל על ממוצע המדגם $\overline{X}$ :
$X_{i}=1$ פירושו שבחזרה ה $i$ התקבלה תוצאה הכלולה ב $A$. לפיכך, הסכום $\sum\limits_{i=1}^{n}X_{i}$ הוא מספר הפעמים שבהן התקבלה תוצאה הכלולה ב$A$. מספר זה הוא השכיחות של $A$ שמסומן גם ב $f(A)$ לכן ממוצע המדגם הוא

$$\overline{X}= \frac{f(A)}{n}= \frac{1}{n}\sum\limits_{i=1}^{n}X_{i}$$

וזוהי בידיוק השכיחות היחסית של $A$ ב $n$ החזרות.
עתה, נתבונן במדגם של $n$ תצפיות מתוך $X$. כלומר, $n$ משתנים מקריים $X_{1}+\dots+ X_{n}$ בלתי תלויים ולכל אחד התפלגות זהה לזו של $X$.
לפי אי שיוויון צבישב יתקיים על ממוצע המדגם:

$$P(\overline{X}\in(p-\varepsilon,p+\varepsilon))\geq 1- \frac{p(1-p)}{\varepsilon^{2}n}$$

נוכל גם לרשום את זה כך 

$$P(|\overline{X}-p|<\varepsilon)\geq 1- \frac{p(1-p)}{\varepsilon^{2}n}$$

ולגבי המאורע המשלים 

$$P(|\overline{X}-p|\geq\varepsilon)\leq \frac{p(1-p)}{\varepsilon^{2}n}$$

נשים לב כי לכל $0\leq p\leq 1$ מתקיים $p(1-p)\leq \frac{1}{4}$ ולכן 

$$P(|\overline{X}-p|\geq\varepsilon)\leq \frac{p(1-p)}{\varepsilon^{2}n}\leq \frac{1}{4\varepsilon^{2}n}$$

ואם נציב במקום $\overline{X}$ את מה שקיבלנו נקבע

$$P\left(\bigg| \frac{1}{n}f(A)-p\bigg|\geq\varepsilon\right)\leq \frac{p(1-p)}{\varepsilon^{2}n}\leq \frac{1}{4\varepsilon^{2}n}$$

במילים: אם מבצעים $n$ חזרות בלתי תלויות על אותו ניסוי, ההסתברות שהשכיחות היחסית של הופעת מאורע מסויים $A$ תהיה שונה מהסתברותו $p$ ביותר מאשר $\varepsilon$ קטנה מהביטוי שבאגף ימין. ביטוי זה קטן ככל שמגדילים את $n$.

_אפשר לומר שעם עליית מספר החזרות, שואפת השכיחות היחסית של הופעת מאורע להסתברותו. עובדה זו נקראת ==התופעה האמפירית== אשר שימשה לנו השראה לבניית המודל של תורת ההסתברות_

## דגימה מתוך התפלגות נורמלית
ציינו שאי-שיוויון צ׳בישב הוא גס למדי ובדרך כלל אינו מעניק מידע מספיק. נרצה להעזר בכלים מתמטיים חזקים יותר כדי לקבל מידע מפורט יותר על האוכלוסייה ממדגם בודד.

כמו כן, חסרון נוסף של הערכות לפי אש״צ היא בכך שאלו הערכות הסתברותיות בטווחים סימטרים סביב התוחלת. לעתים נתעניין בהסתברות מאורעות שאינם מן הצורה הזו דווקא. למשל, נרצה לדעת מהי ההסתברות שממוצע המדגם יהיה במרחק שבין סטיית תקן אחת לבין שתי סטיות תקן מעל לתוחלת האוכלוסייה.

למעשה, היינו רוצים לדעת בנוסף לתוחלת ולסטיית התקן של ממוצע המדגם , גם את פונקציית ההסתברות שלו. באמצעות פונקציית הצפיפות ניתן לחשב בידיוק את ההסתברות של כל מאורע על ממוצע המדגם.
![Screenshot 2023-08-03 at 13.11.46.png|350](/img/user/Assets/Screenshot%202023-08-03%20at%2013.11.46.png)

השטח המקווקו הוא הסתברות כפי שאנחנו מכירים מ[[Computer Science/Probability/Continuous Random Variables\|משתנים מקריים רציפים]] והשטח המקווקו בתמונה למעלה אינו ניתן להערכה באמצעות אש״צ.

קשה מאוד לחשב את פונקציית ההתפלגות אבל במקרה מיוחד של [[Computer Science/Probability/Normal Random Variable\|התפלגות נורמלית]] הדבר אכן אפשרי.

__תזכורת: התפלגות נורמלית__
זאת פונקציה דמויית פעמון שיש לה שני פרמטרים: תוחלת $\mu$ וסטיית תקן $\sigma$. היא סימטרית סביב התוחלת וככל שסטיית התקן גדלה, העקומה נעשית נמוכה ושטוחה יותר.
![Pasted image 20230104022744.png|300](/img/user/Assets/Pasted%20image%2020230104022744.png)
מכל התפלגות נורמלית של משתנה מקרי $X$ בעלת תוחלת $\mu$ וסטיית תקן $\sigma$ אפשר לעבור להתפלגות נורמלית סטנדרטית (תוחלת $0$ וסטיית תקן $1$).
נוכל לעשות זאת באמצעות ציון תקן $Z_{X}= \frac{X-\mu}{\sigma}$. 
נזכיר גם שמסמנים ב $\Phi(z)$ את ההתפלגות המצטברת של התפלגות נורמלית סטנדרטית. וישנה טבלת ערכים שמאפשרת לנו לחשב זאת 

![Pasted image 20230104022924.png|600](/img/user/Assets/Pasted%20image%2020230104022924.png)
כמו כן, נזכיר את התכונה ש $\phi(z)=1-\phi(-z)$ .

>[!info] משפט
בדגימת מדגם שגודלו $n$ מתוך מ״מ נורמלי $X$ בעל תוחלת $\mu$ ושונות $\sigma$ יהיה ממוצע המדגם $\overline{X}$ גם הוא מ״מ נורמלי, בעל תוחלת $\mu$ ושונות $\frac{\sigma^{2}}{n}$  
כלומר: $X\sim N(\mu,\sigma^{2})\to\overline{X}\sim N(\mu, \frac{\sigma^{2}}{n})$

כלומר נוכל לקחת פרמטר מתפלג נורמלי מהאוכלוסייה ויתקיים עבור ממוצע המדגם שלו $\overline{X}\sim N(\mu, \frac{\sigma^{2}}{n})$ ואת זה נוכל לנרמל ולקבל $Z_\overline{X} = \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1)$


![Screenshot 2023-08-03 at 13.33.05.png|450](/img/user/Assets/Screenshot%202023-08-03%20at%2013.33.05.png)

## משפט הגבול המרכזי 
ראינו כמה דברים שאפשר לומר על התפלגות הדגימה של $\overline{X}$:
	א. תוחלת התפלגות זו שווה לתוחלת $X$, כלומר $E(\overline{X})=E(X)$
	ב. שונות התפלגות זו שווה לשונות $X$ חלקי גודל המדגם $n$
	ג. ממוצע המדגם מתפלג נורמלית אם $X$ מתפלג נורמלית.

מסתבר, שגם אם $X$ אינו מתפלג נורמלית עדיין ההתפלגות של $\overline{X}$ קרובה עד מאוד להתפלגות נורמלית בתנאי שנבחר גודל מדגם $n$ גדול במידה מספקת.

הרעיון זה שאם נבחר מדגם גדול ונחזור על הדגימה פעמים רבות מאוד ונרשום בכל פעם את $\overline{X}$, אזי כאשר נערוך את כל התוצאות $\overline{X}$ בהיסטורגמה, היא תהיה קרובה מאוד להתפלגות נורמלית.

>[!info] משפט הגבול המרכזי
יהי $X$ מ״מ כלשהו בעל תוחלת $\mu$ וסטיית תקן $\sigma$ , ויהיו $X_{1},X_{2},\dots,X_{n}$ מ״מ בלתי תלויים בעלי התפלגות זהה ל $X$ . אזי ההתפלגות של הממוצע  $\overline{X}= \frac{1}{n}(X_{1}+\dots+X_{n})$ שואפת ל $N(\mu,\frac{\sigma}{\sqrt{n}})$ כאשר $n$ שואף ל $\infty$.

במונחים של גבולות נקבל 
$$\lim_{n\to\infty} P\left( \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}<\alpha\right)= \Phi(\alpha)$$

זהו אחד המשפטים ההסתברותיים החשוב ביותר לסטטיסטיקה. 
הוא מאפשר לנו לחשב הסתברויות הנוגעות לממוצע המדגם הלקוח מתוך מ״מ או מאוכלוסייה כלשהי, גם כאשר המשתנה הנחקר אינו בעל התפלגות נורמלית דווקא, כפי שקורה ברוב המקרים המעשיים. _באופן מעשי מתברר שגודל מספיק הוא יותר מ 30._
