---
{"dateCreated":"2024-05-25 23:04","tags":["machine_learning"],"pageDirection":"rtl","dg-publish":true,"permalink":"/computer-science/artificial-intelligence/machine-learning/generalization/","dgPassFrontmatter":true}
---

# Generalization
הכללה בלמידת מכונה מתייחסת ליכולת של מודל לבצע ביצועים טובים על נתונים חדשים, בלתי נראים, שאינם חלק ממערך ההדרכה. מודל שמכליל היטב הוא כזה שיכול לבצע במדויק תחזיות או החלטות על סמך תשומות חדשות, במקום רק לשנן את נתוני האימון.

ישנם מספר מתווים עיקריים יכולת כזאת:
1) supervised learning - פונקציה הממפה קלט לפלט.
2) unsupervised learning - מידע לא מאורגן
3) reinforcement learning - למידה מסביבה אינטראקטיבית.


## Supervised learning 
בהינתן אוסף של דוגמאות $(x_{i},y_{i})$ כאשר x הוא פלט ו y הוא קלט. הבעיה בונה פונקציה הממפה בין __הפלט לקלט__ $h:x \to y$ סט אימון. 

__המטרה:__ להפעיל את הפונקציה על קלט חדש $h(x_{test})\approx y_{test}$ בסבירות גבוהה. 

הפונקציה h נבחרת ממשפחה של פונקציות H כל פונקציה נקבעת על ידי פרמטרים שונים W. נרצה למצוא את הפרמטרים המתאימים ביותר לפונקציה הטובה ביותר.

![Screenshot 2024-05-25 at 23.20.13.png](/img/user/Assets/Screenshot%202024-05-25%20at%2023.20.13.png)
בעצם נרצה למצוא את הגרף הירוק (מייצג את ה truth) והנקודות הכחולות מייצגות את הערך של f(x) עם סטיית ״רעש״. המטרה היא בעצם למצוא את הפונקציה הירוקה לפי הערכים הכחולים. כלומר __מציאת הכלל__ לפי הדוגמאות.
### polynomial
משפחת הפונקציות הפולינומיות יכולה לעזור לנו להשיג קירוב טוב לכל פונקציה. זאת משפחה של פונקציות שיכולה לעזור לנו לתאר התנהגות של data, משפחה ״אקספרסיבית״. כיוון שבפועל אנחנו לא יודעים מה הכלל אנחנו צריכים לבחור משפחה מספיק עשירה שנוכל להשתמש בה על הדוגמאות שלנו כדי לקרב את הדוגמאות לכלל.

בmachine learning בעית הלמידה היא למצוא ערכים טובים למקדמים. בהינתן מודל w נגדיר $h_{w}(x)= \sum_{j} w_{j}x^{j}$ .

על ידי קביעת המקדמים נוכל לקבוע האם הפונקציה שלנו קרובה לדוגמאות (חישוב הפרשים)
![Screenshot 2024-05-25 at 23.37.44.png](/img/user/Assets/Screenshot%202024-05-25%20at%2023.37.44.png)

את השגיאה נגדיר באופן הבא

$$E(w)= \frac{1}{2} \sum_{i=1}^{N}(h_{w}(x_{i})-y_{i})^{2}$$

הביטוי $(h_{w}(x_{i})-y_{i})^{2}$ מסומן כ $l(w;x_{i},y_{i})$ ומוגדר כ loss function.

ישנן מספר סיבות לבחירת פונקצית השגיאה הזאת. בעיקר בגלל הנחת ההתפלגות של הdata והנוחות לעבוד עם הפרש ריבועי.

נראה כיצד למצוא את הערכים האופטימליים של המקדמים לפי הדרגה של המודל. לשם הדוגמה לקחנו את הפונקציה sin(x). 

הרעיון הוא פשוט __לצמצם את השגיאה__ או למצוא עבור אילו ערכי w אנחנו מקבלים את השגיאה המינימלית. כלומר נרצה לגזור ולהשוות ל 0 לפי כל אחד מהמקדמים.

אם הדרגה היא 0 - כלומר מתקיים $h_{w}(x)=w_{0}$ 

$$E(w) = \frac{1}{2}\sum(w_{0} - y_{i})^{2}$$

נגזור 

$$\frac{dE}{dw_{0}}= \sum w_{0} - \sum y_{i} =0 \to w_{0}=\frac{1}{n} \sum y_{i}$$

בעצם כאשר הערך של $w_{0}$ הוא הממוצע של הנקודות נקבל שגיאה מינימלית.

![Screenshot 2024-05-26 at 0.03.19.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.03.19.png)

באופן דומה כאשר הדרגה היא 1 נוכל למצוא את את המקדמים האופטימליים כאשר $h_{w}(x)= w_{0}+ w_{1}x$ כעת שוב פעם גוזרים לפי כל אחד מהמקדמים

![Screenshot 2024-05-26 at 0.07.20.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.07.20.png)

נשים לב לתופעה מעניינת, ככל שמעלים את הפולינום השגיאות ביחס ל training set (הנקודות הכחולות) יקטן והדיוק ישתפר. אבל הביצועים על הקו הירוק נעשים גרועים.

![Screenshot 2024-05-26 at 0.12.37.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.12.37.png)

כלומר ישנן 2 בעיות, או שהביצועים לא טובים במצב שהדרגה נמוכה מדי או שעשינו יותר מדי אופטימיזציות על ידי דרגה גבוהה מדי.

![Screenshot 2024-05-26 at 0.14.20.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.14.20.png)

הפער הזה שנקרא overfitting מייצג את העובדה שבדרך כלל הdata שהמודל עובד איתו , כולל בתוכו גם איזה bias או רעש וככל שעולים בדרגת הפולינום המודל לומד גם את הרעש הזה ולכן בסופו של דבר הוא לא ייצג את הכלל האמיתי. 

__הפתרון הפשוט ביותר__ הוא להוסיף עוד data:
למשל עבור 15 דגימות פולינום ממעלה 9 יתנהג כך-
![Screenshot 2024-05-26 at 0.19.51.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.19.51.png)

ועבור 100 דגימות- 
![Screenshot 2024-05-26 at 0.20.28.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.20.28.png)

כלומר תמיד אפשר לשפר את המידע על ידי מתן עוד data. 

הפתרון השני הוא __generalization__ 
אם נסתכל על ערכי w ככל שהדרגה עולה נראה שהם מקבלים מספרים מאוד גבוהים מה שיוצר את הפונקציות המסורבלות שראינו. 

![Screenshot 2024-05-26 at 0.35.21.png](/img/user/Assets/Screenshot%202024-05-26%20at%200.35.21.png)

הרעיון כעת אומר שנרצה למצוא שתי מרכיבים לבעיה הראשון זה השגיאה והשני הוא מרכיב שבודק את הגודל המשוואות של w. נגדיר:

$$\overparen{E}(w)= \frac{1}{2}\sum_{i=1}^{N}l(w;x_{i},y_{i}) + \frac{\lambda}{2}\mid\mid w\mid\mid^{2}$$
החלק השני נקרא regularizer 
מתקיים ש 

$$\mid\mid w\mid\mid^{2}= w^{T}w = \sum_{i=0}^{d} w_{i^{2}}$$

$\lambda$ הוא פרמטר שקובע את המשקל כאשר הוא קטן הבעיה דומה לבעיה הקודמת אך כאשר הוא ערך גבוה  השאיפה היא ש $|| w||^{2} = 0$ . כלומר אנחנו משלמים מחיר על ה training loss לעומת פשטות המודל. כלומר ככל שנותנים משקל גדול לregulator אז אכפת לנו פחות מהdata ויותר שהמודל יהיה פשוט ונקבל פתרונות שדומים לפתרון עבור פולינום מדרגה 0.. 
### סוגי שגיאות
Training Error (1 - שגיאה אמפירית. נרצה שיהיה נמוך
Test Error (2 - שגיאה אמפירית. טיב המודל. בחירת hyper parameter שנותן מינימום test error.
3 ) generalization error - לא יודעים את הכלל. אין אינסוף מידע. התוחלת תניב את התוצאה בקירוב.