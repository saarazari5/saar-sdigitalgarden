---
dateCreated: "2022-11-06 14:46"
tags: [probability, computer_science]
dg-publish: true
dg-home: false
---

# משפט ההסתברות השלמה וחוק בייס
משפט ההסתברות השלמה וחוקי בייס נובעים מחוקי ה[[הסתברות מותנת]] .

## משפט ההסתברות השלמה 
יהי $A_{1},\dots,A_{n}$ מאורעות זרים שמהווים __חלוקה__ של מרחב המדגם (במילים אחרות, כל תוצאה אפשרית שייכת לבידיוק אחת מהמאורעות הנ״ל). נניח גם ש $\forall_{i\in[n]}: P(A_{i})>0$ . אזי למאורע $B$ יתקיים:
$$P(B)= \sum\limits_{i=1}^{n}P(A_{i}\cap B)= \sum\limits_{i=1}^{n}P(A_{i})P(B|A_{i})$$
החוק הזה מאוד שימושי במדעי המחשב שכן משתמש בו כדי לחשב הסתברויות של מאורעות שונים בשיטת [[Divide and conquer]] .
באופן אינטואיטיבי אנחנו מחלקים את מרחב המדגם לחלוקה של מספר מאורעות, לאחר מכן מכניסים את המאורע $B$ והוא יהיה הממוצע המשוקלל של ההסתברויות המותנת שלו תחת המאורעות שמהווים חלוקה (נוכל לבחור חלוקה כזאת בעצמנו ולנצל את התכונה הזאת).
![[Pasted image 20221106150140.png|450]]
התרשים בצד ימין מתאר את האניטואצייה שדיברתי עליה למעלה והתרשים בצד שמאל מסביר למה הנוסחה שמתארת את $P(B)$ נכונה.

### דוגמאות לשימוש במשפט ההסתברו תהשלמה
1)  נכנסת לטורניר שחמט וההסתברות שלך לנצח במשחק שם היא 0.3 מול חצי מהשחקים (נקרא להם t1). 0.4 נגד רבע מהשחקים (t2) ו 0.5 נגד הרבע הנותרים. נבחר שחקן באקראי מולו תתמודד, מה הסיכוי שלך לנצח 
נגדיר את החלוקה להיות מהאורעות $A_{1,2,3}$ שאלו המאורעות לשחק מול שחקן מסוג מסויים.
וכעת נגדיר את מאורע הנצחון $B$
אנחנו יודעים מהו $P(A_{i})$ לפי היחס של כל אחד מסוגי השחקנים ביחס לכולם. ואנחנו יודעים מהי ההסתברות של $P(B|A_{i})$ כי נתון.
מכאן נוכל להשתמש בנוסחה ולקבל 
$$P(B)= \sum\limits_{i=1}^{3}P(A_{i}\cap B)= \sum\limits_{i=1}^{3}P(A_{i})P(B|A_{i})= 0.375$$

2)  מטילים קובייה הוגנת עם 4 פאות. אם התוצאה היא $1$ או $2$ אז מטילים פעם אחת נוספת , אחרת מפסיקים. מהי ההסתברות שסכום ההטלות הוא לפחות 4?
בגלל שהקובייה הוגנת אז לכל מספר שיצא ההסתברות היא $\frac{1}{4}$ בהטלה הראשונה .  נגדיר מאורע $B$ שהוא מה שנרצה לחשב. 
נגדיר $A_{i}$ המאורע שיצא $i$ בהטלה הראשונה. כעת יתקיים 
$$P(B|A_{1})= \frac{1}{2}, P(B|A_{2})= \frac{3}{4}, P(B|A_{3})= 0, P(B|A_{4})=1$$
ומפה לא בעיה לחשב.


## חוק בייס
נוסחת ההסתברו תהשלמה באה ביחד עם המשפט הבא שמתקשר גם הוא להסתברות מותנת. החוק הזה מקשר בין $P(B|A)$ עם $P(A|B)$ כלומר ההסתברות המותנת ההפוכה.
יהי $A_{1},\dots,A_{n}$ מאורעות זרים שמהווים __חלוקה__ של מרחב המדגם (במילים אחרות, כל תוצאה אפשרית שייכת לבידיוק אחת מהמאורעות הנ״ל). נניח גם ש $\forall_{i\in[n]}: P(A_{i})>0$ . אזי למאורע $B$ שהסתברותו גדולה מ0 יתקיים:
$$P(A_{i}|B)= \frac{P(A_{i})P(B|A_{i})}{P(B)}=\frac{P(A_{i})P(B|A_{i})}{\sum\limits_{i=1}^{n}P(A_{i})P(B|A_{i})}$$
![[Pasted image 20221106160256.png|450]]
דוגמה לשימוש בחוק בייס: 
התמונה מתארת צילום x-ray של אדם עם גידול $B$ זה מאורע שמתאר את האפקט שאותו אדם חווה ומרחב המדגם מחולק לכל המאורעות האפשריים שיכולים להיות לו.
בהנחה ואנחנו יודעים מה ההסתברויות המותנות נרצה לקבל את ההסתברות למחלה $A_{i}$ בהינתן הסימפטום $B$. נוכל לגלות לפי חוק בייס בידיוק את זה. 

