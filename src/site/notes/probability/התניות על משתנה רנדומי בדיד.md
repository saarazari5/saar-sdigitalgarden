---
dateCreated: "2022-12-08 21:56"
tags: [probability, computer_science]
dg-publish: true
dg-home: false
---

# התניות על משתנה רנדומי בדיד

אנחנו כבר יודעים ש[[הסתברות מותנת]] יכולה לשמש אותנו כדי לתפוס מידע שמועבר על ידי מספר מאורעות על ההסתברויות השונות של משתנה רנדומי. אם כן, נרצה להציג התנייה על פונקציית מסת ההסתברות, שבהינתן שמאורע מסויים התרחש או בהינתן שיש לנו ערך מסויים של משתנה רנדומי אחר. 

## התנייה של משתנה רנדומי עם מאורע
ה PMF המותנה של משתנה רנדומי $X$ מותנה במאורע כלשהו $A$ עם $P(A)>0$ מוגדר על ידי 
$$p_{X|A}(x)= P(X=x|A)= \frac{P(\{X=x\}\cap A)}{P(A)}$$
כמובן שלכל $x$ נקבל תוצאה אחרת אבל נקבל 
$$P(A)=\sum\limits_{x}P(\{X=x\}\cap A)$$
זה נובע מ[[משפט ההסתברות השלמה וחוק בייס]]. 

סך הכל נקבל :
$$\sum\limits_{x}p_{X|A}(x)= \sum\limits_{x}\frac{P(\{X=x\}\cap A)}{P(A)} = \frac{1}{P(A)}\sum\limits_{x}P(\{X=x\}\cap A)= \frac{P(A)}{P(A)}=1$$

המסקנה המתבקשת היא ש $p_{X|A}$ היא פונקציית מסת הסתברות תקינה.
החישוב של PMF מותנה די דומה לגרסה הלא מותנת שלא, 
כדי להשיג את $p_{X|A}(x)$ נסכום את כל ההסתברויות שיתנו $X=x$ __וגם__ שנמצאות במרחב של $A$ .לאחר הסכימה נבצע נורמליזצייה על ידי חילוק ב $P(A)$ .

![[Pasted image 20221208235043.png|350]]

__דוגמה__ : תלמיד ייקח מבחן באופן חוזר עד $n$ פעמים. כל פעם ההסתברות למעבר היא $p$ בלי תלות למספר הפעמים שנבחן קודם לכן. 
בהינתן שהתלמיד עבר את המבחן בניסיון כלשהו, מהי פונקציית מסת ההסתברות על מספר הניסיונות.

נגדיר את $A$ להיות המאורע שהסטודנט עבר את המבחן (לכל היותר בניסיון ה $n$). נציג משתנה רנדומי $X$ שמייצג את מספר הניסיונות הדרושים כדי לעבור אם מספר לא מוגבל של ניסיונות היה מותר. 
במצב זה $X$ הוא משתנה גיאומטרי עם פרמטרים $p$ והמאורע $A=ֿ\{ X\leq n \}$  ויתקיים
$$P(A)=\sum\limits_{m=1}^{n}(1-p)^{m-1}p$$
נשים לב שהסכימה כאן היא בגלל ש$n$ הוא חסם עליון ולא מספר הניסיונות המדוייק שצריך לעשות עד להצלחה.
אם כן , יתקיים ש 
$$p_{X|A}(k)= \begin{cases} \frac{(1-p)^{k-1}p}{\sum\limits_{m=1}^{n}(1-p)^{m-1}p} & k\in [1,n]\\ 
 \\ 0 & else
\end{cases}$$

המונה בביטוי שהבאנו הוא בידיוק $P(A\cap \{X=k\})$ כי החיתוך של כל הניסיונות עד $n$ עם פונקציית מסת ההסתברות של המשתנה הגיאומטרי שהייתה הצלחה ב $k$ בידיוק היא עדיין המשתנה הגיאומטרי שכן הוא מוכל במאורע $A$ .

![[Pasted image 20221209122841.png|350]]

## התנייה של משתנה רנדומי עם משתנה רנדומי אחר
יהי $X,Y$ שתי משתנים רנדומיים המשוייכים לאותו ניסוי. 
אם אנחנו יודעים שהערך של $Y$ הוא ערך כלשהו $y$ עם פונקציית מסת התסברות גדולה מ0, נוכל לקבל ידע חלקי על הערך של $X$. הידע הזה נתפס על ידי פונקציית מסת ההסתברות המותנת $p_{X|Y}$ של $X$ בהינתן $Y$. זה מקרה פרטי של [[#התנייה של משתנה רנדומי עם מאורע]] 
$$p_{X|A}\text{ where: } A=\{Y=y\}$$
ולכן יתקיים ש 
$$p_{X|Y}(x|y)= \frac{P(X=x, Y=y)}{P(Y=y)}= \frac{p_{X,Y}(x,y)}{p_{Y}(y)}$$

נראה שזאת אכן פונקציית מסת התסתברות תקינה,
עבור $y$ כלשהו שיקיים $p_{Y}(y)>0$ יתקיים
$$\sum\limits_{x} p_{X|Y}(x|y)= \frac{1}{p_{Y}(y)}\cdot \sum\limits_{x} p_{X,Y}(x,y)= \frac{p_{Y}(y)}{p_{Y}(y)}=1$$
![[Pasted image 20221209144216.png|450]]
כלומר הפונקצייה הזאת היא $PMF$ תקין של $X$ (נשים לב שזה אינטואיבית יכול לבלבל בגלל שלכאורה הpmf המותנה הוא חלק מהpmf הכולל של $X$ אבל החילוק בפונקציית מסת ההתסברות שלנעלם אחר היא זאת שמאפשרת את תכונת הנרמול הזאת).

ה$PMF$ המותנה מאפשר לנו לחשב באופן נוח את ה$PMF$ המשותך של $X,Y$ עבור $x,y$ כלשהם:
$$p_{X,Y}(x,y)= p_{X|Y}(x|y)\cdot p_{Y}(y)$$
או המקביל אליו 
$$p_{X,Y}(x,y)= p_{X}(x)\cdot p_{Y|X}(y|x)$$
זה נובע לגמרי מ[[הסתברות מותנת#חוק הכפל]].

__דוגמה__
הפרופסור באוניברסיטה לעתים קרובות טועה בנתונים ועונה לתלמידים תשובות שגויות בהסתברות של $\frac{1}{4}$, בלי תלות מסויימת בשאלות האחרות.
בכל הרצאה הפרופסור נשאל $0,1,2$ (לכל היותר 2) שאלות עם הסתברות שווה של $\frac{1}{3}$ (ההסתברות לכמה שאלות ישאלו היא $\frac{1}{3}$) .
נגדיר את $X,Y$ להיות מספר השאלות שנשאלו ומספר השאלות שנענו עם טעות בתשובה , בהתאמה. 
נרצה לבנות את פונקציית מסת ההסתברות המשותפת לכל ערכי $x,y$ האפשריים. נוכל לתאר זאת כעץ החלטות באופן הבא
![[Pasted image 20221209160130.png|350]]

לדוגמה עבור המקרה שבו שאלה אחת נענתה שגוי בהינתן ששאלה אחת נשאלה נקבל 
$$p_{X,Y}(1,1)= p_{X}(1)\cdot p_{Y|X}(y|x)= \frac{1}{3}\cdot \frac{1}{4}= \frac{1}{12}= \frac{4}{48}$$
נשים לב שיש לנו כאן סימונים שונים לבעיות שאנחנו כבר מכירים מהסתברות מותנת כי בעצם חישבנו כאן חיתוך בין שתי מאורעות על ידי שימוש בעץ החלטות סדרתי.

נוכל לתאר את התוצאות של ה $PMF$ המשותף בטבלה באופן הבא
![[Pasted image 20221209155908.png|350]]
ומכאן כפי שאנחנו כבר יודעים נוכל לחשב את פונקציית מסת ההסתברות השולית

$$p_{X}(x)= \sum\limits_{y}p_{X,Y}(x,y)= \sum\limits_{y} p_{Y}(y)p_{X|Y}(x|y)$$
שזה בעצם [[משפט ההסתברות השלמה וחוק בייס]]


## תוחלת מותנת
ה $PMF$ המותנה הוא בעצם פונקציית מסת הסתברות סטנדרטית תחת ״יקום״ חדש שנקבע על ידי המאורע המותנה.
באותו נשימה, נוכל להגדיר תוחלת מותנת באופן דומה. היא כמו התוחלת הטריוויאלית רק שהיא כפופה ליקום החדש. כמו כן כל ההסתברויות ופונקציות מסת ההסתברות מוחלפות בגרסאות המותנות שלהן. (שונות מותנת גם כן מתנהגת כך). 

__מספר תכונות חשובות__:
_א)_ התוחלת המותנת של $X$ בהינתן מאורע A עם הסתברות גדולה ממש מ0 מוגדרת להיות
$$E[X|A]= \sum\limits_{x}x \cdot p_{X|A}(x)$$
_ב)_ עבור פונקצייה $g(X)$ יתקיים
$$E[g(x)|A]= \sum\limits_{x}g(x)\cdot p_{X|A}(x)$$
_ג)_ התוחלת המותנת של $X$ בהינתן שקיבלנו ערך $y$ של משתנה רנדומי $Y$ יהיה
$$E[X|Y=y]= \sum\limits_{x}x\cdot p_{X|Y}(x|y)$$
_ד)_ אם $A_{1},A_{2},A_{3},\dots,A_{n}$ הן מאורעות זרים שמהווים חלוקה של מרחב המדגם וכל אחד מהם עם התסברות גדולה מ0 אזי 
$$E[X]=\sum\limits_{i=1}^{n}P(A_{i})E[X|A_{i}]$$

יתרה מכך, לכל מאורע $B$ שיקיים $\forall_{1\leq i\leq n}: P(A_{i}\cap B)>0$   אזי 
$$E[X|B]=\sum\limits_{i=1}^{n}P(A_{i}|B)E[X|A_{i}\cap B]$$

_ה)_ יתקיים בתוצאה מסעיף ד. (כי לכל $y$ המאורעות זרות אחת לשנייה)
$$E[X]= \sum\limits_{y}p_{Y}(y)\cdot E[X|Y=y]$$

שתי המשפטים האחרונים שקולים והם נקראים __משפט התוחלת השלמה__.הרעיון הוא שבדומה להסתברות השלמה שאומרת שמרכיבים את ההסתברות הלא מותנת על ידי סכימה של כל ההסתברויות המותנות שלה. באותו אופן נוכל להוכיח את משפט התוחלת השלמה
$$\displaylines{p_{X}(x)= \sum\limits_{i=1}^{n}P(A_{i})\cdot p_{X|A_{i}}(x) \\ 
xp_{X}(x)= x\cdot\sum\limits_{i=1}^{n}P(A_{i})\cdot p_{X|A_{i}}(x)\\
\sum\limits_{x}xp_{X}(x)= \sum\limits_{x}x\cdot\sum\limits_{i=1}^{n}P(A_{i})\cdot p_{X|A_{i}}(x)\\
E[X]= \sum\limits_{i=1}^{n}P(A_{i})\sum\limits_{x}x\cdot p_{X|A_{i}}(x)=\sum\limits_{i=1}^{n}P(A_{i})E[X|A_{i}]
} $$

## תוחלת ושונות של משתנה גיאומטרי
אנחנו כותבים תוכנה שוב ושוב בהסתברות $p$ שהיא תעבוד כמו שצריך. אין תלות בין ניסיונות. מהי התוחלת והשונות של $X$ , מספר הניסיונות עד שהתוכנה עובדת כמו שצריך.
נזהה ש $X$ הוא [[משתנים רנדומיים-בדיד#משתנה גיאומטרי]] עם פונקציית מסת הסתברות  
$$p_{X}(k)= (1-p)^{k-1}p$$
אם נחשב את התוחלת והשונות נקבל ש 
$$E[X]= \sum\limits_{k=1}^{\infty} k(1-p)^{k-1}\cdot p$$
$$var(X) = \sum\limits_{k=1}^{\infty} (k-E[X])^{2}(1-p)^{k-1}p$$
החישוב של הסכום הזה יכול להיות קשוח מאוד. כאלטרנטיבה נוכל להשתמש בנוסחת התוחלת השלמה עם $A_{1}= \{X=1\}$ כלומר הניסיון הראשון הוא הצלחה ו $A_{2}= \{X>1\}$ כלומר הניסוי הראשון הוא כשלון, ובאופן הזה נקבל חישוב פשוט יותר.

אם כן, נקבל שאם הניסיון הראשון הוא מוצלח אז 
$$E[X|X=1]=1$$
אם הניסיון הראשון נכשל, אז זרקנו ניסיון אחד וחוזרים לאן שהתחלנו, ולכן לאחר הנסיון הראשון נקבל שמספר הניסיונות הנותרים המצופה הוא עדיין $E[X]$ 
$$E[X|X>1]= 1+E[X]$$
הנ״ל נכון לאור העובדה שהערכים ש $X$ מקבל הם אינסופיים (כעוצמת הטבעיים). הרעיון הוא שבגלל שתוחלת היא תלויה בכל הקלטים אז גם אחרי שהזריקה הראשונה נכשלה זה לא ישנה כי אנחנו לא מחפשים הצלחה בנקודה ספיצפית ולכן מרגע הכשלון בעצם אנחנו ממשיכים לזרוק כרגיל בלי חשיבות למה היה קודם, עד להצלחה בזמן לא ידוע.


למשל: אם נזרוק מטבע עד לקבלת H ונרצה לחשב את פונקציית מסת ההסתברות עבור הצלחה בפעם השלישית נקבל
$$p_{X}(3)= P(\{X=3\})= P(T_{1}T_{2}H_{3})= (1-p)^{2}p $$
עם זאת, נניח שההטלה הראשונה הביאה $T$ ונרצה עדיין להצליח בניסיון השלישי נקבל 
$$p_{X-1|X>1}(3)= P(X-1=3| X>1)= P(T_{2}T_{3}H_{4})= (1-p)^{2}p$$
קיבלנו את אותה התוצאה. 
תכונה זאת של המשתנה הגיאומטרי נקראת __חוסר הזכרון__ והיא באה להגיד שאין תלות מתי לנקודת ההתחלה של הניסוי זה תמיד יצא אותה התוצאה (מצב שבו מישהו זרק שלוש הטלות והלך ומישהו אחר בה אחריו ומנסה להגיע להצלחה בזריקה השלישית שלו, הסיכויים של שניהם שווים למרות שכבר עברו 3 הטלות). 

באופן כללי נוכל לנסח את התכונה הזאת כך
$$p_{X-n|X>n}(k)= p_{X}(k)$$

אם כן, נקבל
$$E[X]=P(X=1)E[X|X=1]+ P(X>1)E[X|X>1]= p+ (1-p)(1+E[X])$$
נפשט את המשוואה ונקבל 
$$E[X]= \frac{1}{p}$$

באופן דומה יתקיים
$$E[X^{2}|X=1]=1$$
וגם 
$$E[X^{2}|X>1]= E[(1+X)^{2}]= E[1+2X+X^{2}]= 1+2E[X]+E[X^{2}]$$
בדומה ל $E[X]$ נקבל
$$E[X^{2}]=p\cdot 1+ (1-p)(1+2E[X]+E[X^{2}])$$
ויתקיים
$$E[X^{2}]= \frac{1+2(1-p)E[X]}{p}= \frac{2}{p^{2}}- \frac{1}{p}$$
וסך הכל נקבל
$$var(X)= E[X^{2}]-(E[X])^{2}= \frac{1-p}{p^{2}}$$



